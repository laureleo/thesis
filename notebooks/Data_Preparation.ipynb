{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREPARATION\n",
    "We need to format the suc3.0 data, which exist in an xml tree, in such a way that ALBERT can accept it.\n",
    "\n",
    "Because of my familiarity with dataframes, I'll begin by converting the xml tree into a pandas dataframe\n",
    "\n",
    "\n",
    "I'd recommend this link for an easy read on how to use BERT for NER, which should be a good enough paralell for ALBERT\n",
    "https://www.vamvas.ch/bert-for-ner/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import xml.etree.ElementTree as et \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The attributes at each level of the corpus\n",
    "\n",
    "TEXT_attributes = {\n",
    " 'text_blingbring': '|',\n",
    " 'text_id': '|',\n",
    " 'text_lix': '|',\n",
    " 'text_nk': '|',\n",
    " 'text_ovix': '|',\n",
    " 'text_swefn': '|',\n",
    " 'text_index': 0}\n",
    "\n",
    "SENTENCE_attributes = {\n",
    " 'sentence__geocontext': '|',\n",
    " 'sentence_id': '|',\n",
    " 'sentence_index': 0}\n",
    "\n",
    "WORD_attributes = {\n",
    " 'word_blingbring': '|',\n",
    " 'word_complemgram': '|',\n",
    " 'word_compwf': '|',\n",
    " 'word_dephead': '|',\n",
    " 'word_deprel': '|',\n",
    " 'word_lemma': '|:|',\n",
    " 'word_lex': '|',\n",
    " 'word_msd': '|',\n",
    " 'word_pos': '|',\n",
    " 'word_prefix': '|',\n",
    " 'word_ref': '|',\n",
    " 'word_sense': '|',\n",
    " 'word_suffix': '|',\n",
    " 'word_swefn': '|',\n",
    " 'word_ex': '|',\n",
    " 'word_name': '|',\n",
    " 'word_subtype': '|',\n",
    " 'word_type': '|',\n",
    " 'word_index': 0}\n",
    "\n",
    "# A dict containing all attributes associated with a single word, increading higher-level ones\n",
    "WORD_LEVEL_MASTER_DICT = {}\n",
    "WORD_LEVEL_MASTER_DICT.update(TEXT_attributes)\n",
    "WORD_LEVEL_MASTER_DICT.update(SENTENCE_attributes)\n",
    "WORD_LEVEL_MASTER_DICT.update(WORD_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Some tags/dictionaries have a conflicting namespace for the attributes\n",
    "This function takes the attributes and adds a prefix to the keynames, to make sure nothing overwrites anything else\n",
    "Returns the same dict but with updated key names\n",
    "\"\"\"\n",
    "def rename_attributes(dictionary, prefix):\n",
    "    new_dict = {}\n",
    "    for key in dictionary.attrib:\n",
    "        value = dictionary.get(key)\n",
    "        new_attrib_name = prefix + \"_\" + key\n",
    "        new_dict.update({new_attrib_name:value})\n",
    "        \n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert XML to Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a tree\n",
    "xtree = et.parse(\"../data/suc3.xml\")\n",
    "\n",
    "#Get its root element\n",
    "suc3 = xtree.getroot()\n",
    "\n",
    "#Get all children - the texts that we will extract words from\n",
    "texts = suc3.getchildren()\n",
    "\n",
    "text_index = 0\n",
    "sentence_index = 0\n",
    "word_index = 0\n",
    "\n",
    "\n",
    "#Create a list of datapoints. Each datapoint will be a word with all the attributes of the word itself, the attributes of the sentence it is part of, and the attributes of the text it is part of.\n",
    "datapoints = []\n",
    "\n",
    "# Loop over the texts in the corpus...\n",
    "for text in texts:\n",
    "    #Assign an index based on the order it appears in the corpus\n",
    "    text_index = text_index + 1\n",
    "    \n",
    "    # Create a dict of attributes that won't cause a namespace conflict\n",
    "    text_attributes = rename_attributes(text, 'text')\n",
    "    \n",
    "    #Add the index as an attribute\n",
    "    text_attributes.update({\"text_index\": text_index})\n",
    "    \n",
    "    #Get all the sentences that make up this text\n",
    "    sentences = text.getchildren()  \n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence_index = sentence_index + 1\n",
    "\n",
    "        sentence_attributes = rename_attributes(sentence, 'sentence')\n",
    "        sentence_attributes.update({\"sentence_index\": sentence_index})\n",
    "   \n",
    "        words = sentence.getchildren()\n",
    "        \n",
    "        for word in words:\n",
    "            word_index = word_index + 1\n",
    "            \n",
    "            \n",
    "            #For words, there are multiple kinds of attributes depending on the kind of word.\n",
    "            # WORD_attributes is simply a list of all these attributes joined into one.\n",
    "            word_attributes = WORD_attributes.copy()\n",
    "            word_attributes.update({\"word_index\": word_index})\n",
    "            \n",
    "            attributes = rename_attributes(word, 'word')\n",
    "            \n",
    "            if word.tag == 'ne':\n",
    "                attributes.update({\"word_tag\": \"ne\"})\n",
    "            else:\n",
    "                attributes.update({\"word_tag\": \"w\"})\n",
    "                \n",
    "                #If it's not a named entity, the text is not stored as an attribute but actual text. We treat it as an attribute for coherence\n",
    "                attributes.update({\"word_name\": word.text})\n",
    "                \n",
    "            word_attributes.update(attributes)\n",
    "            \n",
    "            #Copy the master dict and fill it in with all the information we've extracted from the three loops\n",
    "            WORD_LEVEL_DICT = WORD_LEVEL_MASTER_DICT.copy()\n",
    "            \n",
    "            WORD_LEVEL_DICT.update(text_attributes)\n",
    "            WORD_LEVEL_DICT.update(sentence_attributes)\n",
    "            WORD_LEVEL_DICT.update(word_attributes)\n",
    "            \n",
    "            datapoints.append(WORD_LEVEL_DICT)\n",
    "                \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace | tokens with NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Having extracted all words and their associated information, let's convert it into a dataframe\n",
    "df = pd.DataFrame(datapoints)\n",
    "\n",
    "#Blingbring looks less interesting\n",
    "df = df.drop(columns={'text_blingbring'})\n",
    "\n",
    "#Replace the | with None\n",
    "df = df.replace({'|': np.nan})\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the data\n",
    "#df.to_pickle('../data/suc3_dataframe')\n",
    "df = pd.read_pickle('../data/suc3_dataframe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract only the parts relevant for NER with ALBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Starting out, I wasn't certain which fields I was going to need, so I grabbed them all.\n",
    "#Now we narrow things down\n",
    "data = df[['sentence_index', 'word_name', 'word_pos', 'word_type', 'word_subtype']]\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace the Nan type of non-entities with O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The example uses O instaed of Nan, so we follow them\n",
    "data[['word_type']] = data[['word_type']].replace(np.nan,'O')\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Switch to wordpiece representation of words using the albert tokenizer\n",
    "\n",
    "Wordpiece segmentation has some advantages. Even when a model is pretrained and has a good starting point from transfer learning, some words are just too rare to represent correctly.\n",
    "Wordpiece segmentation breaks unknown words down into smaller subparts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"KB/albert-base-swedish-cased-alpha\")\n",
    "\n",
    "#An example of wordpiece segmentation. The whole word might be unknown, but splitting the word into known parts may allow the model to leverage its understanding anyway.\n",
    "tokenizer.tokenize('Sovjetledarens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do it for all the data. This takes a while.\n",
    "# Each word gets represented as a list of the parts it was split into\n",
    "data['wordpiece'] = data.apply(lambda x: tokenizer.tokenize(x.word_name), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Switch to BIO representation of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bio_labels(dataframe_row):\n",
    "    \"\"\"\n",
    "    Switch from single-word single-label to word-piece -bio-piece-labels\n",
    "    This is done by, for each word, taking the list of subwords it was split into, and matching labels to that.\n",
    "    We will unravel the lists later\n",
    "    \"\"\"\n",
    "    length_of_split = len(dataframe_row.wordpiece)\n",
    "    current_tag = dataframe_row.word_type\n",
    "\n",
    "    labelpiece = []\n",
    "\n",
    "    #The first part of the split words gets tagged with an \"B-\" for beginning\n",
    "    labelpiece.append(\"B-\" + current_tag)\n",
    "\n",
    "    for i in range(1, length_of_split):   \n",
    "        #The rest of the parts get tagged with an \"I-\" for intermediate\n",
    "        labelpiece.append(\"I-\" + current_tag)\n",
    "\n",
    "    return labelpiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the above function to all the data\n",
    "data['labelpiece'] = data.apply(lambda x: generate_bio_labels(x), axis = 1)\n",
    "data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete all nans in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have obsereved errors caused by the word name \\n in the data, so we ignore all those rows\n",
    "rows_with_errors = data[data['word_name'] == '\\n'].shape[0]\n",
    "total_rows = data.shape[0]\n",
    "print(f\"Dropping all tokens consiting of '/\\ n' as these are incorrectly tagged. We lose {rows_with_errors/total_rows} percent of tokens this way\")\n",
    "\n",
    "data = data[data['word_name'] != '\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look at a random sentence\n",
    "data[data['sentence_index'] == 22]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group tokens and labels by sentence index. Add special tokens\n",
    "The special tokens are [CLS] and [SEP], used as the very first and last token in a sentence. [CLS], token1, token2... tokenn, [SEP]\n",
    "\n",
    "Their use seem to be limited in NER - CLS for example will be used to store an embedding for the whole sentence - but let's add them anyway since everyone else does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It seems we're only gonna need the wordpieces and labelpieces\n",
    "subset = data[['sentence_index','wordpiece', 'labelpiece']].copy()\n",
    "\n",
    "#Group by sentence\n",
    "sentence_groupings = subset.groupby('sentence_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_groups = []\n",
    "\n",
    "#Get each group and put it into a list instead of a dict, for my own convenience\n",
    "for sentence_index in sentence_groupings.groups.keys():\n",
    "    # Get the data\n",
    "    sentence_group = sentence_groupings.get_group(sentence_index)\n",
    "    sentence_groups.append(sentence_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We obtain one element for each sentence. This element is the word-piece sentence\n",
    "#And of course, a corresponding label\n",
    "# This takes a while\n",
    "wordpiece_sentences = []\n",
    "wordpiece_labels = []\n",
    "\n",
    "for sentence in sentence_groups:\n",
    "\n",
    "    tokenized_sentence = []\n",
    "    tokenized_labels = []\n",
    "    \n",
    "    #Add CLS token\n",
    "    tokenized_sentence.append('[CLS]')\n",
    "    tokenized_labels.append('[CLS]')\n",
    "\n",
    "    for word in range(sentence.shape[0]):\n",
    "        row = sentence.iloc[word]\n",
    "        tokenized_labels.extend(row.labelpiece)\n",
    "        tokenized_sentence.extend(row.wordpiece)\n",
    "    \n",
    "    #Add SEP token\n",
    "    tokenized_sentence.append('[SEP]')\n",
    "    tokenized_labels.append('[SEP]')\n",
    "    \n",
    "    wordpiece_sentences.append(tokenized_sentence)\n",
    "    wordpiece_labels.append(tokenized_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check for errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the length of all sequences and corresponding labels, they should match\n",
    "error_sentences = []\n",
    "for i in range(len(wordpiece_labels)):\n",
    "    labelcount = len(wordpiece_labels[i])\n",
    "    sentencecount = len(wordpiece_sentences[i])\n",
    "    if labelcount != sentencecount:\n",
    "        error_sentences.append(i)\n",
    "        \n",
    "\n",
    "print(f\"There are length errors in {len(error_sentences)/len(wordpiece_labels)} percent of sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def look_at_error_index(index, error_sentences):\n",
    "    sentence_to_look_at = index\n",
    "    error_index = error_sentences[sentence_to_look_at]\n",
    "\n",
    "    print(wordpiece_sentences[error_index])\n",
    "    print(wordpiece_labels[error_index])\n",
    "    # What is actually wrong?\n",
    "    for i in range(len(wordpiece_sentences[error_index])):\n",
    "        print(wordpiece_sentences[error_index][i] + \" : \" + wordpiece_labels[error_index][i])\n",
    "\n",
    "    #Look at the original sentences\n",
    "    data[data['sentence_index'] == error_index +1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes on observed errors\n",
    "# \\n has wordtype person (sentence 55). We go back to fix that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to dataframes, store for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to a dataframe, since those pleas me\n",
    "wordpiece_sentences_df = pd.DataFrame(wordpiece_sentences)\n",
    "wordpiece_labels_df = pd.DataFrame(wordpiece_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store them for later use\n",
    "wordpiece_sentences_df.to_pickle('../data/wordpiece_sentences_df')\n",
    "wordpiece_labels_df.to_pickle('../data/wordpiece_labels_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = pd.read_pickle('../data/wordpiece_sentences_df')\n",
    "labels = pd.read_pickle('../data/wordpiece_labels_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(sentences.head())\n",
    "display(labels.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace None with \\<pad\\>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The tokenizer can't deal with None. We replace it with '<pad>', which the albert tokenizer encodes as padding by encoding it as 0\n",
    "# This will mean that the albert model will ignore these items, which is what we want\n",
    "print(tokenizer.encode('<pad>'))\n",
    "sentences.fillna('<pad>', inplace=True)\n",
    "labels.fillna('<pad>', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expand tokenizer vocabulary to be able to deal with our tags\n",
    "The tags aren't native to swedish ner - the model won't have a representation for B-PER for example.\n",
    "\n",
    "So we add those manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We're working with labels ['B-LOC/ORG', 'B-EVN', 'B-LOC/PRS', 'I-LOC/ORG', 'I-TME', 'I-ORG', 'I-PRS', 'B-ORG/PRS', 'I-PRS/WRK', 'B-O', 'B-LOC', 'B-ORG', 'I-OBJ', 'B-LOC/LOC', 'B-MSR', '[SEP]', 'I-LOC', 'B-PRS/WRK', 'I-EVN', 'B-PRS', 'B-OBJ/ORG', 'I-ORG/PRS', '<pad>', 'I-O', 'I-OBJ/ORG', 'I-MSR', 'B-OBJ', 'B-WRK', '[CLS]', 'I-WRK', 'B-TME', 'I-LOC/LOC', 'I-LOC/PRS']\n"
     ]
    }
   ],
   "source": [
    "# Grab all the tags we've created\n",
    "label_set = set()\n",
    "for column in labels.columns:\n",
    "    unique_labels = labels[column].unique()\n",
    "    for label in unique_labels:\n",
    "        label_set.add(label)\n",
    "label_list = list(label_set)\n",
    "\n",
    "print(f\"We're working with labels {label_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing how the tokenizer encodes our labels right now\n",
      "Encoded version of labels with current tokenizer [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1]\n",
      "Decoded version of labels with current tokenizer <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>[SEP]<unk><unk><unk><unk><unk><unk><pad><unk><unk><unk><unk><unk>[CLS]<unk><unk><unk><unk>\n",
      "Yeah that's not gonna work\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing how the tokenizer encodes our labels right now\")\n",
    "encoded = tokenizer.encode(label_list, add_special_tokens=False)\n",
    "print(f\"Encoded version of labels with current tokenizer {encoded}\")\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(f\"Decoded version of labels with current tokenizer {decoded}\")\n",
    "print(\"Yeah that's not gonna work\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing how the tokenizer encodes our labels right now\n",
      "Encoded version of labels with current tokenizer [50000, 50001, 50002, 50003, 50004, 50005, 50006, 50007, 50008, 50009, 50010, 50011, 50012, 50013, 50014, 3, 50015, 50016, 50017, 50018, 50019, 50020, 0, 50021, 50022, 50023, 50024, 50025, 2, 50026, 50027, 50028, 50029]\n",
      "Decoded version of labels with current tokenizer B-LOC/ORG B-EVN B-LOC/PRS I-LOC/ORG I-TME I-ORG I-PRS B-ORG/PRS I-PRS/WRK B-O B-LOC B-ORG I-OBJ B-LOC/LOC B-MSR [SEP] I-LOC B-PRS/WRK I-EVN B-PRS B-OBJ/ORG I-ORG/PRS <pad> I-O I-OBJ/ORG I-MSR B-OBJ B-WRK [CLS] I-WRK B-TME I-LOC/LOC I-LOC/PRS\n",
      "Yeah that's  gonna work\n"
     ]
    }
   ],
   "source": [
    "#Add them\n",
    "tokenizer.add_tokens(label_list)\n",
    "print(\"Testing how the tokenizer encodes our labels right now\")\n",
    "encoded = tokenizer.encode(label_list, add_special_tokens=False)\n",
    "print(f\"Encoded version of labels with current tokenizer {encoded}\")\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(f\"Decoded version of labels with current tokenizer {decoded}\")\n",
    "print(\"Yeah that's  gonna work\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Let's save the labels as well\n",
    "np.save('../data/label_list', label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An important note\n",
    "Adjusting the vocabulary of the tokenizer does not translate to the albert model being able to deal with that, we need to tell the model that we've adjusted the amount of embeddings with\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "After updating it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check how the conversion to integers looks right now\n",
    "Integer conversion is needed because the model won't work with charcters - it works with numbers.\n",
    "\n",
    "So we want each token to be represented by an id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_without_special_symbol(token):\n",
    "    \"\"\"\n",
    "    I'm not savy enough to know how to apply the function with only some arguments in the pandas mapping.\n",
    "    I\"m sure there is a way.\n",
    "    But this is fast\n",
    "    \n",
    "    Encodes a single token without special tokens added\n",
    "    \"\"\"\n",
    "    return tokenizer.encode(token, add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(s.head().applymap(encode_without_special_symbol))\n",
    "display(l.head().applymap(encode_without_special_symbol))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It seems like some tokens are split into multiple tokens. Bother, let's have a look at why that is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode('sakademin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode([783, 85, 625])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode([783, 85])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode([625])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so apparently the encode version isn't the best one, it actually performs tokenization by itself. Since I've already split into tokens - though apparently not the same way as this function would - I have another one to use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_tokens_to_ids('sakademin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yes, this looks better.\n",
    "# The conversion takes a while though\n",
    "sentences = sentences.applymap(tokenizer.convert_tokens_to_ids)\n",
    "labels = labels.applymap(tokenizer.convert_tokens_to_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create attentions masks\n",
    "Attention is 0 if the token is entirely unimportant. This is useful for dealing with padding, and is part of the input to the albert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attentionmapping(x):\n",
    "    if x != 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = sentences.applymap(attentionmapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store them for later use\n",
    "#sentences.to_pickle('../data/sentence_ints')\n",
    "#labels.to_pickle('../data/label_ints')\n",
    "#attention.to_pickle('../data/attention_ints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Took a break, don't want to run through all the previous code\n",
    "def restart():\n",
    "    import pandas as pd\n",
    "    from transformers import AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"KB/albert-base-swedish-cased-alpha\")\n",
    "    sentences = pd.read_pickle('../data/sentence_ints')\n",
    "    labels = pd.read_pickle('../data/label_ints')\n",
    "    attentions = pd.read_pickle('../data/attention_ints')\n",
    "    return tokenizer, sentences, labels\n",
    "tokenizer, sentences, labels, attentions = restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with the huge filesize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#Load the data\n",
    "df = pd.read_pickle(\"../data/sentence_labels\")\n",
    "\n",
    "#Work on a subset for testing purposes\n",
    "input_df = df[['Sentence']].copy()\n",
    "label_df = df[['Labels']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb1 = np.load('../data/emb1.npy', mmap_mode='r')\n",
    "emb2 = np.load('../data/emb2.npy', mmap_mode='r')\n",
    "emb3 = np.load('../data/emb3.npy', mmap_mode='r')\n",
    "emb4 = np.load('../data/emb4.npy', mmap_mode='r')\n",
    "print(emb4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Actually the whole embedding matrix is way too big to keep in RAM. The SUC3 embedding takes 23GB, in fact\n",
    "So I can only keep 40 000 wor\n",
    "\n",
    "The size of the final embedding matrix, nevertheless, is quite big.\n",
    "In fact, the size of the whole embedding matrix takes 23gb, so I broke it up into smaller pieces\n",
    "\n",
    "emb2 = get_embeddings_with_gpu_batch(data_tensor_matrix[20000:40000], mask_tensor_matrix[20000:40000], 50)\n",
    "np.save('../data/emb2', emb2)\n",
    "\n",
    "\n",
    "Don't forget to remove the memory footprint emb2 =3 between each loop\n",
    "x = np.load('../data/em')\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Create a file with a shape capable of holding all the embedded data.\n",
    "#a = np.memmap('test.mymemmap', dtype='float64', mode='w+', shape=(74245,50,768))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Join the separated embedding matrices into a single one\n",
    "\n",
    "Use nmap_mode = 'r' to NOT put it into RAM\n",
    "\"\"\"\n",
    "emb1 = np.load('../data/emb1.npy', mmap_mode='r')\n",
    "emb2 = np.load('../data/emb2.npy', mmap_mode='r')\n",
    "emb3 = np.load('../data/emb3.npy', mmap_mode='r')\n",
    "emb4 = np.load('../data/emb4.npy', mmap_mode='r')\n",
    "#lab1 = label_matrix[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We've generated embeddings batch by batch since our ram can't hold it all\n",
    "Now we load those saved batches from memory into a single file\n",
    "\"\"\"\n",
    "\n",
    "def load_embedded_data_into_mmap()\n",
    "    a[0:20000] = emb1\n",
    "    a[20000:40000] = emb2\n",
    "    a[40000:60000] = emb3\n",
    "    a[60000::] = emb4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb4[1] == a[60001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"../data/suc3_formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some analysis of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"../data/sentence_labels\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"KB/bert-base-swedish-cased-ner\")\n",
    "\n",
    "# use WORD tokenization\n",
    "df['SENTENCE_LENGTH'] = df.apply(lambda x: len(tokenizer.encode(x.Sentence)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of words\n",
    "\n",
    "word_bag = []\n",
    "for row in range (df.shape[0]):\n",
    "    s = df.iloc[row].Sentence\n",
    "    word_bag.extend(tokenizer.encode(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(word_bag))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".thesis",
   "language": "python",
   "name": ".thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
