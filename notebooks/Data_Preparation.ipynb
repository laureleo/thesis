{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREPARATION\n",
    "We need to format the suc3.0 data, which exist in an xml tree, in such a way that ALBERT can accept it.\n",
    "\n",
    "Because of my familiarity with dataframes, I'll begin by converting the xml tree into a pandas dataframe\n",
    "\n",
    "\n",
    "Check this link again \n",
    "https://www.vamvas.ch/bert-for-ner/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import xml.etree.ElementTree as et \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The attributes at each level of the corpus\n",
    "\n",
    "TEXT_attributes = {\n",
    " 'text_blingbring': '|',\n",
    " 'text_id': '|',\n",
    " 'text_lix': '|',\n",
    " 'text_nk': '|',\n",
    " 'text_ovix': '|',\n",
    " 'text_swefn': '|',\n",
    " 'text_index': 0}\n",
    "\n",
    "SENTENCE_attributes = {\n",
    " 'sentence__geocontext': '|',\n",
    " 'sentence_id': '|',\n",
    " 'sentence_index': 0}\n",
    "\n",
    "WORD_attributes = {\n",
    " 'word_blingbring': '|',\n",
    " 'word_complemgram': '|',\n",
    " 'word_compwf': '|',\n",
    " 'word_dephead': '|',\n",
    " 'word_deprel': '|',\n",
    " 'word_lemma': '|:|',\n",
    " 'word_lex': '|',\n",
    " 'word_msd': '|',\n",
    " 'word_pos': '|',\n",
    " 'word_prefix': '|',\n",
    " 'word_ref': '|',\n",
    " 'word_sense': '|',\n",
    " 'word_suffix': '|',\n",
    " 'word_swefn': '|',\n",
    " 'word_ex': '|',\n",
    " 'word_name': '|',\n",
    " 'word_subtype': '|',\n",
    " 'word_type': '|',\n",
    " 'word_index': 0}\n",
    "\n",
    "# A dict containing all attributes associated with a single word, increading higher-level ones\n",
    "WORD_LEVEL_MASTER_DICT = {}\n",
    "WORD_LEVEL_MASTER_DICT.update(TEXT_attributes)\n",
    "WORD_LEVEL_MASTER_DICT.update(SENTENCE_attributes)\n",
    "WORD_LEVEL_MASTER_DICT.update(WORD_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Some tags/dictionaries have a conflicting namespace for the attributes\n",
    "This function takes the attributes and adds a prefix to the keynames, to make sure nothing overwrites anything else\n",
    "Returns the same dict but with updated key names\n",
    "\"\"\"\n",
    "def rename_attributes(dictionary, prefix):\n",
    "    new_dict = {}\n",
    "    for key in dictionary.attrib:\n",
    "        value = dictionary.get(key)\n",
    "        new_attrib_name = prefix + \"_\" + key\n",
    "        new_dict.update({new_attrib_name:value})\n",
    "        \n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert XML to Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vic/git/thesis/.thesis/lib/python3.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: This method will be removed in future versions.  Use 'list(elem)' or iteration over elem instead.\n",
      "  \n",
      "/home/vic/git/thesis/.thesis/lib/python3.7/site-packages/ipykernel_launcher.py:30: DeprecationWarning: This method will be removed in future versions.  Use 'list(elem)' or iteration over elem instead.\n",
      "/home/vic/git/thesis/.thesis/lib/python3.7/site-packages/ipykernel_launcher.py:38: DeprecationWarning: This method will be removed in future versions.  Use 'list(elem)' or iteration over elem instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "#Create a tree\n",
    "xtree = et.parse(\"../data/suc3.xml\")\n",
    "\n",
    "#Get its root element\n",
    "suc3 = xtree.getroot()\n",
    "\n",
    "#Get all children - the texts that we will extract words from\n",
    "texts = suc3.getchildren()\n",
    "\n",
    "text_index = 0\n",
    "sentence_index = 0\n",
    "word_index = 0\n",
    "\n",
    "\n",
    "#Create a list of datapoints. Each datapoint will be a word with all the attributes of the word itself, the attributes of the sentence it is part of, and the attributes of the text it is part of.\n",
    "datapoints = []\n",
    "\n",
    "# Loop over the texts in the corpus...\n",
    "for text in texts:\n",
    "    #Assign an index based on the order it appears in the corpus\n",
    "    text_index = text_index + 1\n",
    "    \n",
    "    # Create a dict of attributes that won't cause a namespace conflict\n",
    "    text_attributes = rename_attributes(text, 'text')\n",
    "    \n",
    "    #Add the index as an attribute\n",
    "    text_attributes.update({\"text_index\": text_index})\n",
    "    \n",
    "    #Get all the sentences that make up this text\n",
    "    sentences = text.getchildren()  \n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence_index = sentence_index + 1\n",
    "\n",
    "        sentence_attributes = rename_attributes(sentence, 'sentence')\n",
    "        sentence_attributes.update({\"sentence_index\": sentence_index})\n",
    "   \n",
    "        words = sentence.getchildren()\n",
    "        \n",
    "        for word in words:\n",
    "            word_index = word_index + 1\n",
    "            \n",
    "            \n",
    "            #For words, there are multiple kinds of attributes depending on the kind of word.\n",
    "            # WORD_attributes is simply a list of all these attributes joined into one.\n",
    "            word_attributes = WORD_attributes.copy()\n",
    "            word_attributes.update({\"word_index\": word_index})\n",
    "            \n",
    "            attributes = rename_attributes(word, 'word')\n",
    "            \n",
    "            if word.tag == 'ne':\n",
    "                attributes.update({\"word_tag\": \"ne\"})\n",
    "            else:\n",
    "                attributes.update({\"word_tag\": \"w\"})\n",
    "                \n",
    "                #If it's not a named entity, the text is not stored as an attribute but actual text. We treat it as an attribute for coherence\n",
    "                attributes.update({\"word_name\": word.text})\n",
    "                \n",
    "            word_attributes.update(attributes)\n",
    "            \n",
    "            #Copy the master dict and fill it in with all the information we've extracted from the three loops\n",
    "            WORD_LEVEL_DICT = WORD_LEVEL_MASTER_DICT.copy()\n",
    "            \n",
    "            WORD_LEVEL_DICT.update(text_attributes)\n",
    "            WORD_LEVEL_DICT.update(sentence_attributes)\n",
    "            WORD_LEVEL_DICT.update(word_attributes)\n",
    "            \n",
    "            datapoints.append(WORD_LEVEL_DICT)\n",
    "                \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>text_lix</th>\n",
       "      <th>text_nk</th>\n",
       "      <th>text_ovix</th>\n",
       "      <th>text_swefn</th>\n",
       "      <th>text_index</th>\n",
       "      <th>sentence__geocontext</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>sentence_index</th>\n",
       "      <th>word_blingbring</th>\n",
       "      <th>...</th>\n",
       "      <th>word_swefn</th>\n",
       "      <th>word_ex</th>\n",
       "      <th>word_name</th>\n",
       "      <th>word_subtype</th>\n",
       "      <th>word_type</th>\n",
       "      <th>word_index</th>\n",
       "      <th>word_tag</th>\n",
       "      <th>word_sentiment</th>\n",
       "      <th>word_sentimentclass</th>\n",
       "      <th>word__overlap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aa01c</td>\n",
       "      <td>50.84</td>\n",
       "      <td>1.58</td>\n",
       "      <td>76.88</td>\n",
       "      <td>|Abandonment:95.654|Destroying:87.097|Relation...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>e24e30c0-e24d3ca9</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>w</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  text_id text_lix text_nk text_ovix  \\\n",
       "0   aa01c    50.84    1.58     76.88   \n",
       "\n",
       "                                          text_swefn  text_index  \\\n",
       "0  |Abandonment:95.654|Destroying:87.097|Relation...           1   \n",
       "\n",
       "  sentence__geocontext        sentence_id  sentence_index word_blingbring  \\\n",
       "0                  NaN  e24e30c0-e24d3ca9               1             NaN   \n",
       "\n",
       "   ... word_swefn word_ex word_name word_subtype word_type word_index  \\\n",
       "0  ...        NaN     NaN         I          NaN       NaN          1   \n",
       "\n",
       "  word_tag word_sentiment word_sentimentclass word__overlap  \n",
       "0        w            NaN                 NaN           NaN  \n",
       "\n",
       "[1 rows x 32 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Having extracted all words and their associated information, let's convert it into a dataframe\n",
    "df = pd.DataFrame(datapoints)\n",
    "\n",
    "#Blingbring looks less interesting\n",
    "df = df.drop(columns={'text_blingbring'})\n",
    "\n",
    "#Replace the | with None\n",
    "df = df.replace({'|': np.nan})\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the data\n",
    "df.to_pickle('../data/suc3_dataframe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract only the parts relevant for NER with ALBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_index</th>\n",
       "      <th>word_name</th>\n",
       "      <th>word_pos</th>\n",
       "      <th>word_type</th>\n",
       "      <th>word_subtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>I</td>\n",
       "      <td>PP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>sin</td>\n",
       "      <td>PS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>fÃ¶rsta</td>\n",
       "      <td>RO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>reaktion</td>\n",
       "      <td>NN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>pÃ¥</td>\n",
       "      <td>PP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_index word_name word_pos word_type word_subtype\n",
       "0               1         I       PP       NaN          NaN\n",
       "1               1       sin       PS       NaN          NaN\n",
       "2               1    fÃ¶rsta       RO       NaN          NaN\n",
       "3               1  reaktion       NN       NaN          NaN\n",
       "4               1        pÃ¥       PP       NaN          NaN"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Grab the categories the example use\n",
    "data = df[['sentence_index', 'word_name', 'word_pos', 'word_type', 'word_subtype']]\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace the Nan type of non-entities with O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vic/git/thesis/.thesis/lib/python3.7/site-packages/pandas/core/frame.py:2963: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_index</th>\n",
       "      <th>word_name</th>\n",
       "      <th>word_pos</th>\n",
       "      <th>word_type</th>\n",
       "      <th>word_subtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>I</td>\n",
       "      <td>PP</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>sin</td>\n",
       "      <td>PS</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>fÃ¶rsta</td>\n",
       "      <td>RO</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>reaktion</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>pÃ¥</td>\n",
       "      <td>PP</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_index word_name word_pos word_type word_subtype\n",
       "0               1         I       PP         O          NaN\n",
       "1               1       sin       PS         O          NaN\n",
       "2               1    fÃ¶rsta       RO         O          NaN\n",
       "3               1  reaktion       NN         O          NaN\n",
       "4               1        pÃ¥       PP         O          NaN"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The example uses O instaed of Nan, so we follow them\n",
    "data[['word_type']] = data[['word_type']].replace(np.nan,'O')\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checklist to make the data compatible\n",
    "* Get sentences\n",
    "* Tokenize the sentences\n",
    "* Put wrongly split tokens together.\n",
    "* Transform the tokens to id:s\n",
    "* Define the length L of the input sequence. The longer the sentence the more the context, but also the more processing power needed\n",
    "* Pad sentences shorter than L with 0\n",
    "* Crop sentences longer than L\n",
    "* Tell Albert to ignore padded info (attention_mask = np.where(padded != 0, 1, 0))\n",
    "* Convert to tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FORMAT WORDS/TOKENS\n",
    "* Extract all sentences\n",
    "* Tokenize each sentence\n",
    "* Convert tokens to IDs\n",
    "* Ensure each token sequence has the same length (padding, trunctuating)\n",
    "\n",
    "As a note, Albert and Bert uses a wordpiece tokenizer. This approach has many advantages, though some problems may occur.\n",
    "\n",
    "For example, \"Internet Explorer\" is a single, named entity. Yet the tokenizer will split it into \"Internet\" and Explorer\".\n",
    "\n",
    "For example, a name like Vladmir might get separated into V##, lad##, mir.\n",
    "\n",
    "To deal with that one can use BIO tagging.\n",
    "\n",
    "For each token if it is the B(eginning) of a named entity tag it so, if it is an I(ntermediate) part of a named entity tag it so, if it isn't an entity (Outside), tag it so.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Bert tokenizer often splits words into muliple tokens however, the subparts which starts with ## as seen above.\n",
    "#So we need an additional function to restich the split apart tokens\n",
    "\n",
    "def tokenizer_wrapper(tokenized_text):\n",
    "    corrected_output = []\n",
    "    for token in tokenized_text:\n",
    "        if token.startswith('##'):\n",
    "            corrected_output[-1] += token[2:]\n",
    "        else:\n",
    "            corrected_output.append(token)\n",
    "    return corrected_output\n",
    "            \n",
    "#tokenizer_wrapper(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A class for interacting on a sentence level with the pandas dataframe\n",
    "\"\"\"\n",
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"word_name\"].values.tolist(),\n",
    "                                                           s[\"word_pos\"].values.tolist(),\n",
    "                                                           s[\"word_type\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"sentence_index\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make use of the pre-trained tokenizer from Huggingface\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"KB/bert-base-swedish-cased-ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all sentences\n",
    "getter = SentenceGetter(data)\n",
    "sentences = [\" \".join([s[0] for s in sent]) for sent in getter.sentences]\n",
    "#sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize each sentence\n",
    "tokenized_sentences = []\n",
    "for sentence in sentences:\n",
    "    tokenized_sentences.append(tokenizer.tokenize(sentence))\n",
    "#print(tokenized_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[135, 243, 578, 10540, 68, 3380, 7245, 49796, 27689, 28413, 26922, 49796, 3206, 121, 3393, 4634, 49796, 2901, 6697, 116, 48, 98, 346, 31843, 24926, 671, 4958, 237, 541, 66, 9926, 21667, 36, 16370, 15191, 42, 696, 98, 7]\n"
     ]
    }
   ],
   "source": [
    "#Convert all tokens into ids\n",
    "id_sentences = []\n",
    "for tokenized_sentence in tokenized_sentences:\n",
    "    id_sentences.append(tokenizer.convert_tokens_to_ids(tokenized_sentence))\n",
    "print(id_sentences[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  135   243   578 10540    68  3380  7245 49796 27689 28413 26922 49796\n",
      "  3206   121  3393  4634 49796  2901  6697   116    48    98   346 31843\n",
      " 24926   671  4958   237   541    66  9926 21667    36 16370 15191    42\n",
      "   696    98     7     0     0     0     0     0     0     0     0     0\n",
      "     0     0]\n"
     ]
    }
   ],
   "source": [
    "#Set all sentences to be of the same length by padding and trucating\n",
    "MAXLEN = 50\n",
    "same_size_sequences = pad_sequences(id_sentences, maxlen=MAXLEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "print(same_size_sequences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add labels for each token in each sentence\n",
    "labels = [[s[2] for s in sent] for sent in getter.sentences]\n",
    "\n",
    "tags_vals = list(set(data[\"word_type\"].values))\n",
    "tag2idx = {t: i for i, t in enumerate(tags_vals)}\n",
    "\n",
    "#We have a corresponding list of tags, and do the same thing, converting names to numbers\n",
    "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n",
    "                     maxlen=MAXLEN, value=tag2idx[\"O\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")\n",
    "#print(labels[0])\n",
    "#print(tags[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE ATTENTION MASKS\n",
    "Attention masks will tell BERT how important each token in the sentence is.\n",
    "We put a value of 0 if the token is just padding so that the model ignores it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create attention masks for each sentence\n",
    "\n",
    "# Attention should be 0 if the token is just padding\n",
    "sentence_masks = []\n",
    "for sequence in same_size_sequences:\n",
    "    sentence_mask = []\n",
    "    for token_id in sequence:\n",
    "        sentence_mask.append(float(token_id>0))\n",
    "    sentence_masks.append(sentence_mask)\n",
    "#print(sentence_masks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence\n",
      "Hur Ã¤r det dÃ¥ i MellanÃ¶stern ?\n",
      "\n",
      "Entities\n",
      "['O', 'O', 'O', 'O', 'O', 'LOC', 'O']\n",
      "\n",
      "Padded Token ID\n",
      "[ 1504    54    82   327    31 15894   302     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0]\n",
      "\n",
      "Padded Entity ID\n",
      "[ 7  7  7  7  7 11  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7\n",
      "  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7\n",
      "  7  7]\n",
      "\n",
      "Attention Mask\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Sanity checks\n",
    "s = sentence_to_check = 4\n",
    "\n",
    "print(\"Sentence\")\n",
    "print(sentences[s])\n",
    "print()\n",
    "print(\"Entities\")\n",
    "print(labels[s])\n",
    "print()\n",
    "print(\"Padded Token ID\")\n",
    "print(same_size_sequences[s])\n",
    "print()\n",
    "print(\"Padded Entity ID\")\n",
    "print(tags[s])\n",
    "print()\n",
    "print(\"Attention Mask\")\n",
    "print(sentence_masks[s])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Token_ID</th>\n",
       "      <th>Entities</th>\n",
       "      <th>Entity_ID</th>\n",
       "      <th>Attention_Mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I sin fÃ¶rsta reaktion pÃ¥ Sovjetledarens varnin...</td>\n",
       "      <td>[135, 243, 578, 10540, 68, 3380, 7245, 49796, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, LOC, O, PRS, O, O, O,...</td>\n",
       "      <td>[7, 7, 7, 7, 7, 7, 7, 7, 11, 7, 12, 7, 7, 7, 7...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I en ruta talar en kort rad pÃ¥ ryska om att de...</td>\n",
       "      <td>[135, 59, 17275, 2548, 59, 1337, 1207, 68, 370...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, ...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>- Dels har vi inget index att gÃ¥ efter , vi kr...</td>\n",
       "      <td>[52, 9077, 108, 186, 1696, 9273, 48, 690, 275,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, TME...</td>\n",
       "      <td>[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, ...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>- Men en deporterad blir aldrig fri , sÃ¤ger Ri...</td>\n",
       "      <td>[52, 299, 59, 41611, 103, 444, 1024, 729, 19, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, PRS, O, O, O, O, O...</td>\n",
       "      <td>[7, 7, 7, 7, 7, 7, 7, 7, 7, 12, 7, 7, 7, 7, 7,...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hur Ã¤r det dÃ¥ i MellanÃ¶stern ?</td>\n",
       "      <td>[1504, 54, 82, 327, 31, 15894, 302, 0, 0, 0, 0...</td>\n",
       "      <td>[O, O, O, O, O, LOC, O]</td>\n",
       "      <td>[7, 7, 7, 7, 7, 11, 7, 7, 7, 7, 7, 7, 7, 7, 7,...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74240</th>\n",
       "      <td>Jag var ju Ã¤ndÃ¥ hans favoritsysslingsvÃ¥gerbarn .</td>\n",
       "      <td>[361, 96, 499, 1532, 699, 4349, 10890, 19427, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O]</td>\n",
       "      <td>[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, ...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74241</th>\n",
       "      <td>Du hÃ¶r ju inte ens till slÃ¤kten , du Ã¤r ju ing...</td>\n",
       "      <td>[631, 1009, 499, 127, 2096, 76, 17549, 19, 356...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, ...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74242</th>\n",
       "      <td>\" Det Ã¤r en mycket fin slips .</td>\n",
       "      <td>[98, 160, 54, 59, 408, 577, 26875, 7, 0, 0, 0,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, ...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74243</th>\n",
       "      <td>Det Ã¤r klart att vi ska Ã¤rva .</td>\n",
       "      <td>[160, 54, 1798, 48, 186, 326, 45460, 7, 0, 0, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, ...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74244</th>\n",
       "      <td>Det Ã¤r i samband med arv som slÃ¤ktingarnas rÃ¤t...</td>\n",
       "      <td>[160, 54, 31, 2607, 66, 7447, 67, 14334, 546, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, ...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74245 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Sentence  \\\n",
       "0      I sin fÃ¶rsta reaktion pÃ¥ Sovjetledarens varnin...   \n",
       "1      I en ruta talar en kort rad pÃ¥ ryska om att de...   \n",
       "2      - Dels har vi inget index att gÃ¥ efter , vi kr...   \n",
       "3      - Men en deporterad blir aldrig fri , sÃ¤ger Ri...   \n",
       "4                         Hur Ã¤r det dÃ¥ i MellanÃ¶stern ?   \n",
       "...                                                  ...   \n",
       "74240   Jag var ju Ã¤ndÃ¥ hans favoritsysslingsvÃ¥gerbarn .   \n",
       "74241  Du hÃ¶r ju inte ens till slÃ¤kten , du Ã¤r ju ing...   \n",
       "74242                     \" Det Ã¤r en mycket fin slips .   \n",
       "74243                     Det Ã¤r klart att vi ska Ã¤rva .   \n",
       "74244  Det Ã¤r i samband med arv som slÃ¤ktingarnas rÃ¤t...   \n",
       "\n",
       "                                                Token_ID  \\\n",
       "0      [135, 243, 578, 10540, 68, 3380, 7245, 49796, ...   \n",
       "1      [135, 59, 17275, 2548, 59, 1337, 1207, 68, 370...   \n",
       "2      [52, 9077, 108, 186, 1696, 9273, 48, 690, 275,...   \n",
       "3      [52, 299, 59, 41611, 103, 444, 1024, 729, 19, ...   \n",
       "4      [1504, 54, 82, 327, 31, 15894, 302, 0, 0, 0, 0...   \n",
       "...                                                  ...   \n",
       "74240  [361, 96, 499, 1532, 699, 4349, 10890, 19427, ...   \n",
       "74241  [631, 1009, 499, 127, 2096, 76, 17549, 19, 356...   \n",
       "74242  [98, 160, 54, 59, 408, 577, 26875, 7, 0, 0, 0,...   \n",
       "74243  [160, 54, 1798, 48, 186, 326, 45460, 7, 0, 0, ...   \n",
       "74244  [160, 54, 31, 2607, 66, 7447, 67, 14334, 546, ...   \n",
       "\n",
       "                                                Entities  \\\n",
       "0      [O, O, O, O, O, O, O, O, LOC, O, PRS, O, O, O,...   \n",
       "1      [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "2      [O, O, O, O, O, O, O, O, O, O, O, O, O, O, TME...   \n",
       "3      [O, O, O, O, O, O, O, O, O, PRS, O, O, O, O, O...   \n",
       "4                                [O, O, O, O, O, LOC, O]   \n",
       "...                                                  ...   \n",
       "74240                              [O, O, O, O, O, O, O]   \n",
       "74241         [O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "74242                           [O, O, O, O, O, O, O, O]   \n",
       "74243                           [O, O, O, O, O, O, O, O]   \n",
       "74244            [O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "\n",
       "                                               Entity_ID  \\\n",
       "0      [7, 7, 7, 7, 7, 7, 7, 7, 11, 7, 12, 7, 7, 7, 7...   \n",
       "1      [7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, ...   \n",
       "2      [7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, ...   \n",
       "3      [7, 7, 7, 7, 7, 7, 7, 7, 7, 12, 7, 7, 7, 7, 7,...   \n",
       "4      [7, 7, 7, 7, 7, 11, 7, 7, 7, 7, 7, 7, 7, 7, 7,...   \n",
       "...                                                  ...   \n",
       "74240  [7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, ...   \n",
       "74241  [7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, ...   \n",
       "74242  [7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, ...   \n",
       "74243  [7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, ...   \n",
       "74244  [7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, ...   \n",
       "\n",
       "                                          Attention_Mask  \n",
       "0      [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...  \n",
       "1      [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...  \n",
       "2      [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...  \n",
       "3      [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...  \n",
       "4      [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, ...  \n",
       "...                                                  ...  \n",
       "74240  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...  \n",
       "74241  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...  \n",
       "74242  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, ...  \n",
       "74243  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, ...  \n",
       "74244  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...  \n",
       "\n",
       "[74245 rows x 5 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For convenience, let's pickle this for later use\n",
    "\n",
    "cols=[\"Sentence\", \"Token_ID\", \"Entities\", \"Entity_ID\", \"Attention_Mask\"]\n",
    "\n",
    "data_matrix = []\n",
    "\n",
    "for i in range(0, len(sentences)):\n",
    "    a=sentences[i]\n",
    "    b=same_size_sequences[i]\n",
    "    c=labels[i]\n",
    "    d=tags[i]\n",
    "    e=sentence_masks[i]\n",
    "    \n",
    "    row = [a, b, c, d, e]\n",
    "    data_matrix.append(row)\n",
    "    \n",
    "\n",
    "df = pd.DataFrame(data_matrix, columns=cols)\n",
    "\n",
    "df.to_pickle('../data/suc3_formatted')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actually, it's probably easier to just deal with sentences and labels.\n",
    "Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=[\"Sentence\", \"Labels\"]\n",
    "\n",
    "data_matrix = []\n",
    "\n",
    "for i in range(0, len(sentences)):\n",
    "    a=sentences[i]\n",
    "    b=labels[i]\n",
    "    \n",
    "    row = [a,b]\n",
    "    data_matrix.append(row)\n",
    "    \n",
    "df = pd.DataFrame(data_matrix, columns=cols)\n",
    "\n",
    "df.to_pickle('../data/sentence_labels')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'LOC', 'O', 'PRS', 'O', 'O', 'O', 'O', 'PRS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[ 7  7  7  7  7  7  7  7 11  7 12  7  7  7  7 12  7  7  7  7  7  7  7  7\n",
      "  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7\n",
      "  7  7]\n"
     ]
    }
   ],
   "source": [
    "# Add labels for each token in each sentence\n",
    "labels = [[s[2] for s in sent] for sent in getter.sentences]\n",
    "\n",
    "tags_vals = list(set(data[\"word_type\"].values))\n",
    "tag2idx = {t: i for i, t in enumerate(tags_vals)}\n",
    "\n",
    "#We have a corresponding list of tags, and do the same thing, converting names to numbers\n",
    "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n",
    "                     maxlen=MAXLEN, value=tag2idx[\"O\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")\n",
    "print(labels[0])\n",
    "print(tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original_Pos</th>\n",
       "      <th>Named_Entity</th>\n",
       "      <th>New_Pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Original_Pos, Named_Entity, New_Pos]\n",
       "Index: []"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take the named entities.\n",
    "# Take their indices and keep in a list UNVISITED\n",
    "# For each word in the tokenized sequence\n",
    "\n",
    "# Are we at an index where an NE should be?\n",
    "# If NO, does the word end with ## (indication that it's been split up)\n",
    "# if \n",
    "# If it does not end with ##, continue as per usual.\n",
    "# If it does end with ##,check if it is part of a named entity.\n",
    "# If it is NOT part of a NE, increase the index values for all UNVISITED since a new token has appeard before their original position\n",
    "# Continue\n",
    "# If it IS a part of a NE, join the parts to a single, named \n",
    "\n",
    "label_df = pd.DataFrame(columns={\"Named_Entity\", \"Original_Pos\", \"New_Pos\"})\n",
    "label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_wrapper(tokenized_text):\n",
    "    corrected_output = []\n",
    "    for token in tokenized_text:\n",
    "        if token.startswith('##'):\n",
    "            corrected_output[-1] += token[2:]\n",
    "        else:\n",
    "            corrected_output.append(token)\n",
    "    return corrected_output\n",
    "            \n",
    "#tokenizer_wrapper(tokenized)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original_Pos</th>\n",
       "      <th>Named_Entity</th>\n",
       "      <th>New_Pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Original_Pos, Named_Entity, New_Pos]\n",
       "Index: []"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNVISITED + np.ones(len(UNVISITED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_list = [([s[0] for s in sent]) for sent in getter.sentences]\n",
    "\n",
    "s = sentence_list[0]\n",
    "l = labels[0]\n",
    "t = tokenizer.tokenize(sentences[0])\n",
    "\n",
    "print(s)\n",
    "print()\n",
    "print(l)\n",
    "print()\n",
    "print(t)\n",
    "\n",
    "#TODO SovjetLedaren Ã¤r inte en NE, men nÃ¤r den bryts upp till Sovjet och Ledaren Ã¤r Sovjet en NE\n",
    "ne_indices = []\n",
    "nes = []\n",
    "for i, label in enumerate(l):\n",
    "    if label != 'O':\n",
    "        ne_index = i\n",
    "        ne = s[i]\n",
    "        ne_indices.append(ne_index)\n",
    "        nes.append(ne)\n",
    "\n",
    "print(ne_indices)\n",
    "print(nes)\n",
    "\n",
    "\n",
    "\n",
    "UNVISITED = ne_indices\n",
    "UNVISITED.reverse()\n",
    "ne_index = UNVISITED[0]\n",
    "for i, token in enumerate(t):\n",
    "    \n",
    "    #If we have not reached the named entity\n",
    "    if i < ne_index:\n",
    "        #Check if the current word has been split up\n",
    "        if token.startswith('##'):\n",
    "            #If so, increase the values of all unvisited NE:s, since the indices will have been shifted\n",
    "            continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".thesis",
   "language": "python",
   "name": ".thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
