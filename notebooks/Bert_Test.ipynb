{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Following the guide for BERT as practice\n",
    "https://www.depends-on-the-definition.com/named-entity-recognition-with-bert/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>text_lix</th>\n",
       "      <th>text_nk</th>\n",
       "      <th>text_ovix</th>\n",
       "      <th>text_swefn</th>\n",
       "      <th>text_index</th>\n",
       "      <th>sentence__geocontext</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>sentence_index</th>\n",
       "      <th>word_blingbring</th>\n",
       "      <th>...</th>\n",
       "      <th>word_swefn</th>\n",
       "      <th>word_ex</th>\n",
       "      <th>word_name</th>\n",
       "      <th>word_subtype</th>\n",
       "      <th>word_type</th>\n",
       "      <th>word_index</th>\n",
       "      <th>word_tag</th>\n",
       "      <th>word_sentiment</th>\n",
       "      <th>word_sentimentclass</th>\n",
       "      <th>word__overlap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aa01c</td>\n",
       "      <td>50.84</td>\n",
       "      <td>1.58</td>\n",
       "      <td>76.88</td>\n",
       "      <td>|Abandonment:95.654|Destroying:87.097|Relation...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>e24e30c0-e24d3ca9</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>w</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aa01c</td>\n",
       "      <td>50.84</td>\n",
       "      <td>1.58</td>\n",
       "      <td>76.88</td>\n",
       "      <td>|Abandonment:95.654|Destroying:87.097|Relation...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>e24e30c0-e24d3ca9</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>w</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aa01c</td>\n",
       "      <td>50.84</td>\n",
       "      <td>1.58</td>\n",
       "      <td>76.88</td>\n",
       "      <td>|Abandonment:95.654|Destroying:87.097|Relation...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>e24e30c0-e24d3ca9</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>första</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>w</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aa01c</td>\n",
       "      <td>50.84</td>\n",
       "      <td>1.58</td>\n",
       "      <td>76.88</td>\n",
       "      <td>|Abandonment:95.654|Destroying:87.097|Relation...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>e24e30c0-e24d3ca9</td>\n",
       "      <td>1</td>\n",
       "      <td>|gengäld|gensträvighet|hinder|motstånd|motverk...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>reaktion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>w</td>\n",
       "      <td>-0.247931</td>\n",
       "      <td>neutral</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aa01c</td>\n",
       "      <td>50.84</td>\n",
       "      <td>1.58</td>\n",
       "      <td>76.88</td>\n",
       "      <td>|Abandonment:95.654|Destroying:87.097|Relation...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>e24e30c0-e24d3ca9</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>på</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>w</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>aa01c</td>\n",
       "      <td>50.84</td>\n",
       "      <td>1.58</td>\n",
       "      <td>76.88</td>\n",
       "      <td>|Abandonment:95.654|Destroying:87.097|Relation...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>e24e30c0-e24d3ca9</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sovjetledarens</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>w</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>aa01c</td>\n",
       "      <td>50.84</td>\n",
       "      <td>1.58</td>\n",
       "      <td>76.88</td>\n",
       "      <td>|Abandonment:95.654|Destroying:87.097|Relation...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>e24e30c0-e24d3ca9</td>\n",
       "      <td>1</td>\n",
       "      <td>|anvisning|befallning|bestraffning|betänklighe...</td>\n",
       "      <td>...</td>\n",
       "      <td>|Warning|</td>\n",
       "      <td>NaN</td>\n",
       "      <td>varningar</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>w</td>\n",
       "      <td>-0.5145</td>\n",
       "      <td>negative</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>aa01c</td>\n",
       "      <td>50.84</td>\n",
       "      <td>1.58</td>\n",
       "      <td>76.88</td>\n",
       "      <td>|Abandonment:95.654|Destroying:87.097|Relation...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>e24e30c0-e24d3ca9</td>\n",
       "      <td>1</td>\n",
       "      <td>|bekräftelse|svar|</td>\n",
       "      <td>...</td>\n",
       "      <td>|Statement|</td>\n",
       "      <td>NaN</td>\n",
       "      <td>deklarerade</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>w</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>aa01c</td>\n",
       "      <td>50.84</td>\n",
       "      <td>1.58</td>\n",
       "      <td>76.88</td>\n",
       "      <td>|Abandonment:95.654|Destroying:87.097|Relation...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>e24e30c0-e24d3ca9</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ENAMEX</td>\n",
       "      <td>Litauens</td>\n",
       "      <td>PPL</td>\n",
       "      <td>LOC</td>\n",
       "      <td>9</td>\n",
       "      <td>ne</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>aa01c</td>\n",
       "      <td>50.84</td>\n",
       "      <td>1.58</td>\n",
       "      <td>76.88</td>\n",
       "      <td>|Abandonment:95.654|Destroying:87.097|Relation...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>e24e30c0-e24d3ca9</td>\n",
       "      <td>1</td>\n",
       "      <td>|ledare|myndighet|överordnad|</td>\n",
       "      <td>...</td>\n",
       "      <td>|Leadership|</td>\n",
       "      <td>NaN</td>\n",
       "      <td>president</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>w</td>\n",
       "      <td>-0.0656</td>\n",
       "      <td>neutral</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  text_id text_lix text_nk text_ovix  \\\n",
       "0   aa01c    50.84    1.58     76.88   \n",
       "1   aa01c    50.84    1.58     76.88   \n",
       "2   aa01c    50.84    1.58     76.88   \n",
       "3   aa01c    50.84    1.58     76.88   \n",
       "4   aa01c    50.84    1.58     76.88   \n",
       "5   aa01c    50.84    1.58     76.88   \n",
       "6   aa01c    50.84    1.58     76.88   \n",
       "7   aa01c    50.84    1.58     76.88   \n",
       "8   aa01c    50.84    1.58     76.88   \n",
       "9   aa01c    50.84    1.58     76.88   \n",
       "\n",
       "                                          text_swefn  text_index  \\\n",
       "0  |Abandonment:95.654|Destroying:87.097|Relation...           1   \n",
       "1  |Abandonment:95.654|Destroying:87.097|Relation...           1   \n",
       "2  |Abandonment:95.654|Destroying:87.097|Relation...           1   \n",
       "3  |Abandonment:95.654|Destroying:87.097|Relation...           1   \n",
       "4  |Abandonment:95.654|Destroying:87.097|Relation...           1   \n",
       "5  |Abandonment:95.654|Destroying:87.097|Relation...           1   \n",
       "6  |Abandonment:95.654|Destroying:87.097|Relation...           1   \n",
       "7  |Abandonment:95.654|Destroying:87.097|Relation...           1   \n",
       "8  |Abandonment:95.654|Destroying:87.097|Relation...           1   \n",
       "9  |Abandonment:95.654|Destroying:87.097|Relation...           1   \n",
       "\n",
       "  sentence__geocontext        sentence_id  sentence_index  \\\n",
       "0                  NaN  e24e30c0-e24d3ca9               1   \n",
       "1                  NaN  e24e30c0-e24d3ca9               1   \n",
       "2                  NaN  e24e30c0-e24d3ca9               1   \n",
       "3                  NaN  e24e30c0-e24d3ca9               1   \n",
       "4                  NaN  e24e30c0-e24d3ca9               1   \n",
       "5                  NaN  e24e30c0-e24d3ca9               1   \n",
       "6                  NaN  e24e30c0-e24d3ca9               1   \n",
       "7                  NaN  e24e30c0-e24d3ca9               1   \n",
       "8                  NaN  e24e30c0-e24d3ca9               1   \n",
       "9                  NaN  e24e30c0-e24d3ca9               1   \n",
       "\n",
       "                                     word_blingbring  ...    word_swefn  \\\n",
       "0                                                NaN  ...           NaN   \n",
       "1                                                NaN  ...           NaN   \n",
       "2                                                NaN  ...           NaN   \n",
       "3  |gengäld|gensträvighet|hinder|motstånd|motverk...  ...           NaN   \n",
       "4                                                NaN  ...           NaN   \n",
       "5                                                NaN  ...           NaN   \n",
       "6  |anvisning|befallning|bestraffning|betänklighe...  ...     |Warning|   \n",
       "7                                 |bekräftelse|svar|  ...   |Statement|   \n",
       "8                                                NaN  ...           NaN   \n",
       "9                      |ledare|myndighet|överordnad|  ...  |Leadership|   \n",
       "\n",
       "  word_ex       word_name word_subtype word_type word_index word_tag  \\\n",
       "0     NaN               I          NaN       NaN          1        w   \n",
       "1     NaN             sin          NaN       NaN          2        w   \n",
       "2     NaN          första          NaN       NaN          3        w   \n",
       "3     NaN        reaktion          NaN       NaN          4        w   \n",
       "4     NaN              på          NaN       NaN          5        w   \n",
       "5     NaN  Sovjetledarens          NaN       NaN          6        w   \n",
       "6     NaN       varningar          NaN       NaN          7        w   \n",
       "7     NaN     deklarerade          NaN       NaN          8        w   \n",
       "8  ENAMEX        Litauens          PPL       LOC          9       ne   \n",
       "9     NaN       president          NaN       NaN         10        w   \n",
       "\n",
       "  word_sentiment word_sentimentclass word__overlap  \n",
       "0            NaN                 NaN           NaN  \n",
       "1            NaN                 NaN           NaN  \n",
       "2            NaN                 NaN           NaN  \n",
       "3      -0.247931             neutral           NaN  \n",
       "4            NaN                 NaN           NaN  \n",
       "5            NaN                 NaN           NaN  \n",
       "6        -0.5145            negative           NaN  \n",
       "7            NaN                 NaN           NaN  \n",
       "8            NaN                 NaN           NaN  \n",
       "9        -0.0656             neutral           NaN  \n",
       "\n",
       "[10 rows x 32 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suc3 = pd.read_pickle(\"../data/sucFrame\")\n",
    "suc3.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vic/git/thesis/.thesis/lib/python3.7/site-packages/pandas/core/frame.py:2963: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_index</th>\n",
       "      <th>word_name</th>\n",
       "      <th>word_pos</th>\n",
       "      <th>word_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>I</td>\n",
       "      <td>PP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>sin</td>\n",
       "      <td>PS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>första</td>\n",
       "      <td>RO</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>reaktion</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>på</td>\n",
       "      <td>PP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>Sovjetledarens</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>varningar</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>deklarerade</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>Litauens</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>president</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_index       word_name word_pos word_type\n",
       "0               1               I       PP         O\n",
       "1               1             sin       PS         O\n",
       "2               1          första       RO         O\n",
       "3               1        reaktion       NN         O\n",
       "4               1              på       PP         O\n",
       "5               1  Sovjetledarens       NN         O\n",
       "6               1       varningar       NN         O\n",
       "7               1     deklarerade       VB         O\n",
       "8               1        Litauens      NaN       LOC\n",
       "9               1       president       NN         O"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Grab the categories the example use\n",
    "data = suc3[['sentence_index', 'word_name', 'word_pos', 'word_type']]\n",
    "\n",
    "# The example uses O instaed of Nan, so we follow them\n",
    "data[['word_type']] = data[['word_type']].replace(np.nan,'O')\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"word_name\"].values.tolist(),\n",
    "                                                           s[\"word_pos\"].values.tolist(),\n",
    "                                                           s[\"word_type\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"sentence_index\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "getter = SentenceGetter(data)\n",
    "sentences = [\" \".join([s[0] for s in sent]) for sent in getter.sentences]\n",
    "#sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I sin första reaktion på Sovjetledarens varningar deklarerade Litauens president Vytautas Landsbergis att \" nu avvisar Gorbatjov vår utsträckta hand med extremt skarpa och hämndlystna ord \" .'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'LOC', 'O', 'PRS', 'O', 'O', 'O', 'O', 'PRS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "labels = [[s[2] for s in sent] for sent in getter.sentences]\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_vals = list(set(data[\"word_type\"].values))\n",
    "tag2idx = {t: i for i, t in enumerate(tags_vals)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inst': 0,\n",
       " 'animal': 1,\n",
       " 'PRS/WRK': 2,\n",
       " 'place': 3,\n",
       " 'other': 4,\n",
       " 'LOC/PRS': 5,\n",
       " 'ORG/PRS': 6,\n",
       " 'EVN': 7,\n",
       " 'LOC': 8,\n",
       " 'person': 9,\n",
       " 'TME': 10,\n",
       " 'LOC/LOC': 11,\n",
       " 'myth': 12,\n",
       " 'product': 13,\n",
       " 'PRS': 14,\n",
       " 'ORG': 15,\n",
       " 'WRK': 16,\n",
       " 'O': 17,\n",
       " 'MSR': 18,\n",
       " 'OBJ/ORG': 19,\n",
       " 'work': 20,\n",
       " 'OBJ': 21,\n",
       " 'LOC/ORG': 22,\n",
       " 'event': 23}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Sentence and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "#from pytorch_pretrained_bert import BertForTokenClassification, BertAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertConfig\n",
    "#from transformers import BertForTokenClassification, BertAdam\n",
    "from transformers import BertForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Control sequence length\n",
    "MAX_LEN = 16\n",
    "\n",
    "#batch size\n",
    "bs = 2\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.get_device_name(0) \n",
    "from ipywidgets import IntProgress\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Bert implementation comes with a pretrained tokenizer. This leverages general language understanding.\n",
    "#And is better than rule-based approaches (add refs)\n",
    "#Select the one most suited for your use case. Probably case-based swedish.\n",
    "\n",
    "#TODO how is the pretraining carried out?\n",
    "#TODO exists for swedish?\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'sin', 'for', '##sta', 're', '##ak', '##tion', 'pa', 'so', '##v', '##jet', '##led', '##are', '##ns', 'var', '##ning', '##ar', 'de', '##kla', '##rera', '##de', 'lit', '##au', '##ens', 'president', 'v', '##yt', '##au', '##tas', 'lands', '##berg', '##is', 'at', '##t', '\"', 'nu', 'av', '##vis', '##ar', 'go', '##rba', '##t', '##jo', '##v', 'var', 'ut', '##stra', '##ck', '##ta', 'hand', 'med', 'ex', '##tre', '##mt', 'ska', '##rp', '##a', 'och', 'ham', '##nd', '##ly', '##st', '##na', 'or', '##d', '\"', '.']\n"
     ]
    }
   ],
   "source": [
    "#Python list comprehension. Just tokenize each sentence and put the tokenized sentence in a lsit\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "print(tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Each sentence is a list of words/tokens\n",
    "#Replace each word/token by an id\n",
    "\n",
    "tokens_to_ids = [tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts]\n",
    "\n",
    "#If the number of tokens < maxlen, pad with 0\n",
    "#If the number of tokens > maxlen, cut\n",
    "#This ensures that all vectors have the same length\n",
    "input_ids = pad_sequences(tokens_to_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'sin', 'for', '##sta', 're', '##ak', '##tion', 'pa', 'so', '##v', '##jet', '##led', '##are', '##ns', 'var', '##ning', '##ar', 'de', '##kla', '##rera', '##de', 'lit', '##au', '##ens', 'president', 'v', '##yt', '##au', '##tas', 'lands', '##berg', '##is', 'at', '##t', '\"', 'nu', 'av', '##vis', '##ar', 'go', '##rba', '##t', '##jo', '##v', 'var', 'ut', '##stra', '##ck', '##ta', 'hand', 'med', 'ex', '##tre', '##mt', 'ska', '##rp', '##a', 'och', 'ham', '##nd', '##ly', '##st', '##na', 'or', '##d', '\"', '.']\n",
      "[ 1045  8254  2005  9153  2128  4817  3508  6643  2061  2615 15759  3709\n",
      " 12069  3619 13075  5582]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_texts[0])\n",
    "print(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We have a corresponding list of tags, and do the same thing, converting names to numbers\n",
    "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n",
    "                     maxlen=MAX_LEN, value=tag2idx[\"O\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'LOC', 'O', 'PRS', 'O', 'O', 'O', 'O', 'PRS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[17 17 17 17 17 17 17 17  8 17 14 17 17 17 17 14]\n"
     ]
    }
   ],
   "source": [
    "print(labels[0])\n",
    "print(tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "#Attention masks ensures that padded elements are ignored in the sequences\n",
    "#TODO how. why?\n",
    "attention_masks = [[float(i>0) for i in ii] for ii in input_ids]\n",
    "print(attention_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split to only use 10% of data\n",
    "tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags, \n",
    "                                                            random_state=2018, test_size=0.1)\n",
    "tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2018, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4479  1011  6925 ... 14383 26455  4215]\n",
      " [ 2104  9807  1011 ...  2389 19330  2912]\n",
      " [28166 20014  2063 ...  7412  2063  6655]\n",
      " ...\n",
      " [ 6229  8945  7520 ...  6392  8585 14482]\n",
      " [ 6229  5199  5017 ...  5558  8024 28166]\n",
      " [ 2061  2213  4372 ...  2099  1012     0]]\n",
      "tensor([[ 4479,  1011,  6925,  ..., 14383, 26455,  4215],\n",
      "        [ 2104,  9807,  1011,  ...,  2389, 19330,  2912],\n",
      "        [28166, 20014,  2063,  ...,  7412,  2063,  6655],\n",
      "        ...,\n",
      "        [ 6229,  8945,  7520,  ...,  6392,  8585, 14482],\n",
      "        [ 6229,  5199,  5017,  ...,  5558,  8024, 28166],\n",
      "        [ 2061,  2213,  4372,  ...,  2099,  1012,     0]])\n"
     ]
    }
   ],
   "source": [
    "print(tr_inputs)\n",
    "#Convert everythoing to torch tensors\n",
    "#How does this affect things? TODO the matrices look the same\n",
    "\n",
    "tr_inputs = torch.tensor(tr_inputs)\n",
    "val_inputs = torch.tensor(val_inputs)\n",
    "tr_tags = torch.tensor(tr_tags)\n",
    "val_tags = torch.tensor(val_tags)\n",
    "tr_masks = torch.tensor(tr_masks)\n",
    "val_masks = torch.tensor(val_masks)\n",
    "print(tr_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert it all into a tensordataset \n",
    "\n",
    "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n",
    "\n",
    "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the previous model with a token-level classifier. It's an additional linear layer that takes as input the last hidden state of the sequence\n",
    "\n",
    "model = BertForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(tag2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertConfig\n",
    "\n",
    "# Initializing a BERT bert-base-uncased style configuration\n",
    "configuration = BertConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing a model from the bert-base-uncased style configuration\n",
    "#model = BertModel(configuration)\n",
    "\n",
    "# Accessing the model configuration\n",
    "#configuration = model.config\n",
    "#configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is just for deciding hyperparameters and such\n",
    "\n",
    "FULL_FINETUNING = True\n",
    "if FULL_FINETUNING:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    param_optimizer = list(model.classifier.named_parameters()) \n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=2).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-e41c28a93a91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# add batch to gpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mb_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_input_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-e41c28a93a91>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# add batch to gpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mb_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_input_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "epochs = 5\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "    # TRAIN loop\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # add batch to gpu\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # forward pass\n",
    "        loss, WHAT = model(b_input_ids, token_type_ids=None,\n",
    "                     attention_mask=b_input_mask, labels=b_labels)\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        # track train loss\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "    # print train loss per epoch\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "    # VALIDATION on validation set\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    predictions , true_labels = [], []\n",
    "    for batch in valid_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n",
    "                                  attention_mask=b_input_mask, labels=b_labels)\n",
    "            logits = model(b_input_ids, token_type_ids=None,\n",
    "                           attention_mask=b_input_mask)\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "        true_labels.append(label_ids)\n",
    "        \n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        eval_loss += tmp_eval_loss.mean().item()\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        \n",
    "        nb_eval_examples += b_input_ids.size(0)\n",
    "        nb_eval_steps += 1\n",
    "    eval_loss = eval_loss/nb_eval_steps\n",
    "    print(\"Validation loss: {}\".format(eval_loss))\n",
    "    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
    "    pred_tags = [tags_vals[p_i] for p in predictions for p_i in p]\n",
    "    valid_tags = [tags_vals[l_ii] for l in true_labels for l_i in l for l_ii in l_i]\n",
    "    print(\"F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "for batch in valid_dataloader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n",
    "                              attention_mask=b_input_mask, labels=b_labels)\n",
    "        logits = model(b_input_ids, token_type_ids=None,\n",
    "                       attention_mask=b_input_mask)\n",
    "        \n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    true_labels.append(label_ids)\n",
    "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "\n",
    "    eval_loss += tmp_eval_loss.mean().item()\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    nb_eval_examples += b_input_ids.size(0)\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "pred_tags = [[tags_vals[p_i] for p_i in p] for p in predictions]\n",
    "valid_tags = [[tags_vals[l_ii] for l_ii in l_i] for l in true_labels for l_i in l ]\n",
    "print(\"Validation loss: {}\".format(eval_loss/nb_eval_steps))\n",
    "print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
    "print(\"Validation F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert was too big, let's try another example with distilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/jalammar/jalammar.github.io/blob/master/notebooks/bert/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers as ppb # pytorch transformers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv', delimiter='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "batch_1 = df[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class = ppb.DistilBertModel\n",
    "tokenizer_class = ppb.DistilBertTokenizer\n",
    "pretrained_weights = 'distilbert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model/tokenizer\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and process all sentences together as a batch\n",
    "# That is, replace each unique word with a corresponding id to get a list of lists\n",
    "tokenized = batch_1[0].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pad to ensure the same length\n",
    "\n",
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "\n",
    "np.array(padded).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that input is all the same, tell the model to ignore the padding when it sees it.\n",
    "#This is attention\n",
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to tensors\n",
    "input_ids = torch.tensor(padded)  \n",
    "attention_mask = torch.tensor(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The results of the processing ends up in last_hidden_states.\n",
    "#TODO what does torch.no_grad() do?\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's have a look at the final output\n",
    "\n",
    "\"\"\"\n",
    "We get a 2000 x 59 x 768 tensor\n",
    "\n",
    "The first dimension is the sentence\n",
    "The second dimension is the word\n",
    "The third dimension is the hidden state\n",
    "\n",
    "last_hidden_states[0][:,0,:]\n",
    "All rows (sentences), The first word(the prepended CLS token), all hidden states)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "last_hidden_states[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert classifies sentences by generating a [CLS] (classification) token and prepending it to the output sentences.\n",
    "# This token is an embedding for the entire sentence\n",
    "features = last_hidden_states[0][:,0,:].numpy()\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So we have an embedding for a sentence.\n",
    "#And we have a classification for each sentence.\n",
    "#Then we're back to familiar territory\n",
    "labels = batch_1[1]\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's just use basic logistic regression\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf.score(test_features, test_labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".thesis",
   "language": "python",
   "name": ".thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
