{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bi-lstm test, based on\n",
    "# https://medium.com/illuin/named-entity-recognition-with-bilstm-cnns-632ba83d3d41\n",
    "# https://www.kaggle.com/navya098/bi-lstm-for-ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 281837: expected 25 fields, saw 34\\n'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>lemma</th>\n",
       "      <th>next-lemma</th>\n",
       "      <th>next-next-lemma</th>\n",
       "      <th>next-next-pos</th>\n",
       "      <th>next-next-shape</th>\n",
       "      <th>next-next-word</th>\n",
       "      <th>next-pos</th>\n",
       "      <th>next-shape</th>\n",
       "      <th>next-word</th>\n",
       "      <th>...</th>\n",
       "      <th>prev-prev-lemma</th>\n",
       "      <th>prev-prev-pos</th>\n",
       "      <th>prev-prev-shape</th>\n",
       "      <th>prev-prev-word</th>\n",
       "      <th>prev-shape</th>\n",
       "      <th>prev-word</th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>shape</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>thousand</td>\n",
       "      <td>of</td>\n",
       "      <td>demonstr</td>\n",
       "      <td>NNS</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>IN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>of</td>\n",
       "      <td>...</td>\n",
       "      <td>__start2__</td>\n",
       "      <td>__START2__</td>\n",
       "      <td>wildcard</td>\n",
       "      <td>__START2__</td>\n",
       "      <td>wildcard</td>\n",
       "      <td>__START1__</td>\n",
       "      <td>1.0</td>\n",
       "      <td>capitalized</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>of</td>\n",
       "      <td>demonstr</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>have</td>\n",
       "      <td>NNS</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>...</td>\n",
       "      <td>__start1__</td>\n",
       "      <td>__START1__</td>\n",
       "      <td>wildcard</td>\n",
       "      <td>__START1__</td>\n",
       "      <td>capitalized</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>demonstr</td>\n",
       "      <td>have</td>\n",
       "      <td>march</td>\n",
       "      <td>VBN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBP</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>have</td>\n",
       "      <td>...</td>\n",
       "      <td>thousand</td>\n",
       "      <td>NNS</td>\n",
       "      <td>capitalized</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>of</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>have</td>\n",
       "      <td>march</td>\n",
       "      <td>through</td>\n",
       "      <td>IN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>through</td>\n",
       "      <td>VBN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>marched</td>\n",
       "      <td>...</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>of</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>have</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>march</td>\n",
       "      <td>through</td>\n",
       "      <td>london</td>\n",
       "      <td>NNP</td>\n",
       "      <td>capitalized</td>\n",
       "      <td>London</td>\n",
       "      <td>IN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>through</td>\n",
       "      <td>...</td>\n",
       "      <td>demonstr</td>\n",
       "      <td>NNS</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>have</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>marched</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     lemma next-lemma next-next-lemma next-next-pos  \\\n",
       "0           0  thousand         of        demonstr           NNS   \n",
       "1           1        of   demonstr            have           VBP   \n",
       "2           2  demonstr       have           march           VBN   \n",
       "3           3      have      march         through            IN   \n",
       "4           4     march    through          london           NNP   \n",
       "\n",
       "  next-next-shape next-next-word next-pos next-shape      next-word  ...  \\\n",
       "0       lowercase  demonstrators       IN  lowercase             of  ...   \n",
       "1       lowercase           have      NNS  lowercase  demonstrators  ...   \n",
       "2       lowercase        marched      VBP  lowercase           have  ...   \n",
       "3       lowercase        through      VBN  lowercase        marched  ...   \n",
       "4     capitalized         London       IN  lowercase        through  ...   \n",
       "\n",
       "  prev-prev-lemma prev-prev-pos prev-prev-shape prev-prev-word   prev-shape  \\\n",
       "0      __start2__    __START2__        wildcard     __START2__     wildcard   \n",
       "1      __start1__    __START1__        wildcard     __START1__  capitalized   \n",
       "2        thousand           NNS     capitalized      Thousands    lowercase   \n",
       "3              of            IN       lowercase             of    lowercase   \n",
       "4        demonstr           NNS       lowercase  demonstrators    lowercase   \n",
       "\n",
       "       prev-word sentence_idx        shape           word tag  \n",
       "0     __START1__          1.0  capitalized      Thousands   O  \n",
       "1      Thousands          1.0    lowercase             of   O  \n",
       "2             of          1.0    lowercase  demonstrators   O  \n",
       "3  demonstrators          1.0    lowercase           have   O  \n",
       "4           have          1.0    lowercase        marched   O  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dframe = pd.read_csv(\"./Other/ner.csv\", encoding = \"ISO-8859-1\", error_bad_lines=False)\n",
    "dframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>have</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>marched</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_idx           word tag\n",
       "0           1.0      Thousands   O\n",
       "1           1.0             of   O\n",
       "2           1.0  demonstrators   O\n",
       "3           1.0           have   O\n",
       "4           1.0        marched   O"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1050795 entries, 0 to 1050794\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count    Dtype  \n",
      "---  ------        --------------    -----  \n",
      " 0   sentence_idx  1050794 non-null  float64\n",
      " 1   word          1050794 non-null  object \n",
      " 2   tag           1050794 non-null  object \n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 24.1+ MB\n"
     ]
    }
   ],
   "source": [
    "dataset=dframe.drop(['Unnamed: 0', 'lemma', 'next-lemma', 'next-next-lemma', 'next-next-pos',\n",
    "       'next-next-shape', 'next-next-word', 'next-pos', 'next-shape',\n",
    "       'next-word', 'prev-iob', 'prev-lemma', 'prev-pos',\n",
    "       'prev-prev-iob', 'prev-prev-lemma', 'prev-prev-pos', 'prev-prev-shape',\n",
    "       'prev-prev-word', 'prev-shape', 'prev-word',\"pos\", \"shape\"],axis=1)\n",
    "display(dataset.head())\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        self.n_sent = 1\n",
    "        self.dataset = dataset\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, t) for w,t in zip(s[\"word\"].values.tolist(),\n",
    "                                                        s[\"tag\"].values.tolist())]\n",
    "        self.grouped = self.dataset.groupby(\"sentence_idx\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'O'), ('party', 'O'), ('is', 'O'), ('divided', 'O'), ('over', 'O'), ('Britain', 'B-gpe'), (\"'s\", 'O'), ('participation', 'O'), ('in', 'O'), ('the', 'O'), ('Iraq', 'B-geo'), ('conflict', 'O'), ('and', 'O'), ('the', 'O'), ('continued', 'O'), ('deployment', 'O'), ('of', 'O'), ('8,500', 'O'), ('British', 'B-gpe'), ('troops', 'O'), ('in', 'O'), ('that', 'O'), ('country', 'O'), ('.', 'O'), ('The', 'O'), ('party', 'O'), ('is', 'O'), ('divided', 'O'), ('over', 'O'), ('Britain', 'B-gpe'), (\"'s\", 'O'), ('participation', 'O'), ('in', 'O'), ('the', 'O'), ('Iraq', 'B-geo'), ('conflict', 'O'), ('and', 'O'), ('the', 'O'), ('continued', 'O'), ('deployment', 'O'), ('of', 'O'), ('8,500', 'O'), ('British', 'B-gpe'), ('troops', 'O'), ('in', 'O'), ('that', 'O'), ('country', 'O'), ('.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "getter = SentenceGetter(dataset)\n",
    "sentences = getter.sentences\n",
    "print(sentences[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 140\n"
     ]
    }
   ],
   "source": [
    "maxlen = max([len(s) for s in sentences])\n",
    "print ('Maximum sequence length:', maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.hist([len(s) for s in sentences], bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(set(dataset[\"word\"].values))\n",
    "words.append(\"ENDPAD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30174"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Count how many words there are \n",
    "\n",
    "n_words = len(words)\n",
    "n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = list(set(dataset[\"tag\"].values))\n",
    "n_tags = len(tags)\n",
    "n_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to indices, use a dict\n",
    "word2idx = {w: i for i, w in enumerate(words)}\n",
    "tag2idx = {t: i for i, t in enumerate(tags)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22910"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx['Cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag2idx[\"O\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "X = [[word2idx[w[0]] for w in s] for s in sentences]\n",
    "X = pad_sequences(maxlen=140, sequences=X, padding=\"post\",value=n_words - 1)\n",
    "y = [[tag2idx[w[1]] for w in s] for s in sentences]\n",
    "y = pad_sequences(maxlen=140, sequences=y, padding=\"post\", value=tag2idx[\"O\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "y = [to_categorical(i, num_classes=n_tags) for i in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28141, 140, 18)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(y_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Input\n",
    "\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape\n",
    "y_train[0].shape\n",
    "\n",
    "#For each sentence, output has shape 140 x 18\n",
    "# The full label matrix has shape SENTENCE_IDX, WORD_IDX, ONE_HOT_ENCODED_CLASS\n",
    "#                                        2000x140,18\n",
    "\n",
    "OUT_DIM = y_train[0].shape[1]\n",
    "IN_DIM = TOKENS_IN_SENTENCE = X_train.shape[1]\n",
    "n_words = len(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28141, 140)\n",
      "(28141, 140, 18)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(np.array(y_train).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from tqdm import tqdm, trange\n",
    "from transformers import AutoModel\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from transformers import AutoTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_pickle(\"../data/sentence_labels\")\n",
    "\n",
    "#Work on a subset for testing purposes\n",
    "input_df = df[['Sentence']].head(20000).copy()\n",
    "label_df = df[['Labels']].head(20000).copy()\n",
    "\n",
    "#Load swedish tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"KB/bert-base-swedish-cased-ner\")\n",
    "\n",
    "# Use WORDPIECE tokenization\n",
    "#input_df['Tokenized'] = input_df[['Sentence']].apply(lambda x: tokenizer.tokenize(x[0]), axis=1)\n",
    "# Ensure no named entities have been split apart\n",
    "\n",
    "# use WORD tokenization\n",
    "input_df['Tokenized'] = input_df[['Sentence']].apply(lambda x: x[0], axis=1)\n",
    "\n",
    "# Replace words with integers, add the special [CLS] and [SEP] tokens\n",
    "input_df['Integerized'] = input_df[['Tokenized']].apply(lambda x: tokenizer.encode(x[0], add_special_tokens=True), axis=1)\n",
    "\n",
    "# Pad and truncate all sentences so they are the same length\n",
    "length = 50\n",
    "input_df['Input'] = input_df[['Integerized']].apply(lambda x: pad_sequences(x, maxlen=length, dtype=\"long\", truncating=\"post\", padding=\"post\")[0],       axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = np.array([x[0] for x in input_df[['Input']].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df['Tokenized'] = label_df[['Labels']].apply(lambda x: x[0], axis=1)\n",
    "\n",
    "# Replace labels with integers, add the special [CLS] and [SEP] tokens\n",
    "label_df['Integerized'] = label_df[['Tokenized']].apply(lambda x: tokenizer.encode(x[0], add_special_tokens=True), axis=1)\n",
    "\n",
    "# Pad and truncate all labels so they are the same length\n",
    "length = 50\n",
    "label_df['Output_Class'] = label_df[['Integerized']].apply(lambda x: pad_sequences(x, maxlen=length, dtype=\"long\", truncating=\"post\",                    padding=\"post\")[0], axis=1)\n",
    "\n",
    "#Create the output_class matrix. Doesn't need to be a tensor, since it won't go into BERT\n",
    "label_matrix = np.array([x[0] for x in label_df[['Output_Class']].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = np.array(pd.get_dummies(label_matrix.reshape(-1,))).reshape(label_matrix.shape[0], label_matrix.shape[1], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 50) (20000, 50, 9)\n",
      "[    2   135   243   578 10540    68  3380  7245 49796 27689 28413 26922\n",
      " 49796  3206   121  3393  4634 49796  2901  6697   116    48    98   346\n",
      " 31843 24926   671  4958   237   541    66  9926 21667    36 16370 15191\n",
      "    42   696    98     7     3     0     0     0     0     0     0     0\n",
      "     0     0]\n",
      "[[0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(input_data.shape, output_data.shape)\n",
    "print(input_data[0])\n",
    "print(output_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28141, 140)\n",
      "(28141, 140, 18)\n",
      "[14162 15448 26539 14287 30017  3349  6988  9167  2921 20415  6975 15448\n",
      " 26060 20079  3800 15969 20828 22813  1427  9167  1269 17011  2198 26060\n",
      "  7420 29057 28181 30017 22041 22133 24033 14162 15448 26539 14287 30017\n",
      "  3349  6988  9167  2921 20415  6975 15448 26060 20079  3800 15969 20828\n",
      " 22813  1427  9167  1269 17011  2198 26060  7420 29057 28181 30017 22041\n",
      " 22133 24033 30173 30173 30173 30173 30173 30173 30173 30173 30173 30173\n",
      " 30173 30173 30173 30173 30173 30173 30173 30173 30173 30173 30173 30173\n",
      " 30173 30173 30173 30173 30173 30173 30173 30173 30173 30173 30173 30173\n",
      " 30173 30173 30173 30173 30173 30173 30173 30173 30173 30173 30173 30173\n",
      " 30173 30173 30173 30173 30173 30173 30173 30173 30173 30173 30173 30173\n",
      " 30173 30173 30173 30173 30173 30173 30173 30173 30173 30173 30173 30173\n",
      " 30173 30173 30173 30173 30173 30173 30173 30173]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_train[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140, 18)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_data[0].shape\n",
    "y_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 140)               0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 140, 18)           543132    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 140, 18)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 140, 200)          95200     \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 140, 18)           3618      \n",
      "=================================================================\n",
      "Total params: 641,950\n",
      "Trainable params: 641,950\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def bilstm_x():\n",
    "    # The input is two-dimensional. Each row is a sentence. Each column is a word in the sentence.\n",
    "\n",
    "#For the model, each datapoint is thus a 140-dimensional input \n",
    "\n",
    "    input = Input(shape=(50,))\n",
    "    model = Embedding(input_dim=40441, output_dim=output_data[2], input_length=50)(input)\n",
    "    model = Dropout(0.1)(model)\n",
    "    model = Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1))(model)\n",
    "    out = TimeDistributed(Dense(output_data[2], activation=\"softmax\"))(model)  # softmax output layer\n",
    "    model = Model(input, out)\n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't convert Python sequence with mixed types to Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-726b1ff4686e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbilstm_x\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-3122db1034af>\u001b[0m in \u001b[0;36mbilstm_x\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40441\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBidirectional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecurrent_dropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/thesis/.thesis/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/thesis/.thesis/lib/python3.7/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    461\u001b[0m                                          \u001b[0;34m'You can build it manually via: '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m                                          '`layer.build(batch_input_shape)`')\n\u001b[0;32m--> 463\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/thesis/.thesis/lib/python3.7/site-packages/keras/layers/embeddings.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mregularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings_regularizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings_constraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             dtype=self.dtype)\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/thesis/.thesis/lib/python3.7/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         weight = K.variable(initializer(shape, dtype=dtype),\n\u001b[0m\u001b[1;32m    280\u001b[0m                             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m                             \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/thesis/.thesis/lib/python3.7/site-packages/keras/initializers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, shape, dtype)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         x = K.random_uniform(shape, self.minval, self.maxval,\n\u001b[0;32m--> 115\u001b[0;31m                              dtype=dtype, seed=self.seed)\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/thesis/.thesis/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[0;34m(shape, minval, maxval, dtype, seed)\u001b[0m\n\u001b[1;32m   4355\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4356\u001b[0m         return tf_keras_backend.random_uniform(\n\u001b[0;32m-> 4357\u001b[0;31m             shape, minval=minval, maxval=maxval, dtype=dtype, seed=seed)\n\u001b[0m\u001b[1;32m   4358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/thesis/.thesis/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[0;34m(shape, minval, maxval, dtype, seed)\u001b[0m\n\u001b[1;32m   5624\u001b[0m     \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5625\u001b[0m   return random_ops.random_uniform(\n\u001b[0;32m-> 5626\u001b[0;31m       shape, minval=minval, maxval=maxval, dtype=dtype, seed=seed)\n\u001b[0m\u001b[1;32m   5627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/thesis/.thesis/lib/python3.7/site-packages/tensorflow_core/python/ops/random_ops.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[0;34m(shape, minval, maxval, dtype, seed, name)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0mmaxval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"random_uniform\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxval\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m     \u001b[0mminval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"min\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0mmaxval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"max\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/thesis/.thesis/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mshape_tensor\u001b[0;34m(shape)\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# not convertible to Tensors becasue of mixed content.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m       \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdimension_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"shape\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    995\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/thesis/.thesis/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1314\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/thesis/.thesis/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    315\u001b[0m                                          as_ref=False):\n\u001b[1;32m    316\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/thesis/.thesis/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    256\u001b[0m   \"\"\"\n\u001b[1;32m    257\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 258\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/thesis/.thesis/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    264\u001b[0m   \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/thesis/.thesis/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Can't convert Python sequence with mixed types to Tensor."
     ]
    }
   ],
   "source": [
    "model = bilstm_x()\n",
    "model.fit(input_data, output_data, batch_size=16, epochs=1, validation_split=0.2, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(list_of_models):\n",
    "    for model in list_of_models:\n",
    "        #Train it\n",
    "        print(model.summary())\n",
    "        model.fit(X_train, np.array(y_train), batch_size=16, epochs=1, validation_split=0.2, verbose=1)\n",
    "\n",
    "        print('\\n# Evaluate on test data')\n",
    "        results = model.evaluate(np.array(X_test), np.array(y_test))\n",
    "        for i, metric in enumerate(model.metrics_names):\n",
    "            print(f\"{metric}: {results[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140 30174 18\n"
     ]
    }
   ],
   "source": [
    "print(TOKENS_IN_SENTENCE, n_words, OUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 140)               0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 140, 18)           543132    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 140, 18)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 140, 200)          95200     \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 140, 18)           3618      \n",
      "=================================================================\n",
      "Total params: 641,950\n",
      "Trainable params: 641,950\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def bilstm_1():\n",
    "    # The input is two-dimensional. Each row is a sentence. Each column is a word in the sentence.\n",
    "\n",
    "#For the model, each datapoint is thus a 140-dimensional input \n",
    "\n",
    "    input = Input(shape=(TOKENS_IN_SENTENCE,))\n",
    "    model = Embedding(input_dim=n_words, output_dim=OUT_DIM, input_length=TOKENS_IN_SENTENCE)(input)\n",
    "    model = Dropout(0.1)(model)\n",
    "    model = Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1))(model)\n",
    "    out = TimeDistributed(Dense(OUT_DIM, activation=\"softmax\"))(model)  # softmax output layer\n",
    "    model = Model(input, out)\n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vic/git/thesis/.thesis/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22512 samples, validate on 5629 samples\n",
      "Epoch 1/1\n",
      "22512/22512 [==============================] - 289s 13ms/step - loss: 0.1253 - accuracy: 0.9731 - val_loss: 0.0604 - val_accuracy: 0.9827\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fe4e1db3f50>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = bilstm_1()\n",
    "model.fit(X_train, y_train, batch_size=16, epochs=1, validation_split=0.2, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Evaluate on test data\n",
      "7036/7036 [==============================] - 17s 2ms/step\n",
      "loss: 0.0614988464330529\n",
      "accuracy: 0.9821895360946655\n"
     ]
    }
   ],
   "source": [
    "print('\\n# Evaluate on test data')\n",
    "results = model.evaluate(np.array(X_test), np.array(y_test))\n",
    "for i, metric in enumerate(model.metrics_names):\n",
    "    print(f\"{metric}: {results[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(list_of_models):\n",
    "    for model in list_of_models:\n",
    "        #Train it\n",
    "        print(model.summary())\n",
    "        model.fit(X_train, np.array(y_train), batch_size=16, epochs=1, validation_split=0.2, verbose=1)\n",
    "\n",
    "        print('\\n# Evaluate on test data')\n",
    "        results = model.evaluate(np.array(X_test), np.array(y_test))\n",
    "        for i, metric in enumerate(model.metrics_names):\n",
    "            print(f\"{metric}: {results[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(np.array(y_train).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "to_test = [m1,m2, m3, m4]\n",
    "evaluate_models(to_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_models(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "p = model.predict(np.array([X_test[i]]))\n",
    "p = np.argmax(p, axis=-1)\n",
    "print(\"{:14} ({:5}): {}\".format(\"Word\", \"True\", \"Pred\"))\n",
    "for w,pred in zip(X_test[i],p[0]):\n",
    "    print(\"{:14}: {}\".format(words[w],tags[pred]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".thesis",
   "language": "python",
   "name": ".thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
