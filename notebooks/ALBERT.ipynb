{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data\n",
    "df = pd.read_pickle(\"../data/sentence_labels\")\n",
    "\n",
    "#Work on a subset for testing purposes\n",
    "input_df = df[['Sentence']].head(20000).copy()\n",
    "label_df = df[['Labels']].head(20000).copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting the input and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Format the input\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from transformers import AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load swedish tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"KB/bert-base-swedish-cased-ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use WORDPIECE tokenization\n",
    "#input_df['Tokenized'] = input_df[['Sentence']].apply(lambda x: tokenizer.tokenize(x[0]), axis=1)\n",
    "# Ensure no named entities have been split apart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use WORD tokenization\n",
    "input_df['Tokenized'] = input_df[['Sentence']].apply(lambda x: x[0], axis=1)\n",
    "label_df['Tokenized'] = label_df[['Labels']].apply(lambda x: x[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace words with integers, add the special [CLS] and [SEP] tokens\n",
    "input_df['Integerized'] = input_df[['Tokenized']].apply(lambda x: tokenizer.encode(x[0], add_special_tokens=True), axis=1)\n",
    "label_df['Integerized'] = label_df[['Tokenized']].apply(lambda x: tokenizer.encode(x[0], add_special_tokens=True), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad and truncate all sentences so they are the same length\n",
    "length = 50\n",
    "input_df['Input'] = input_df[['Integerized']].apply(lambda x: pad_sequences(x, maxlen=length, dtype=\"long\", truncating=\"post\", padding=\"post\")[0], axis=1)\n",
    "label_df['Output_Class'] = label_df[['Integerized']].apply(lambda x: pad_sequences(x, maxlen=length, dtype=\"long\", truncating=\"post\", padding=\"post\")[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add attention mask. Attention is 0 for padding, else 1\n",
    "input_df['Attention_Mask'] = input_df[['Input']].apply(lambda x: (x[0] != 0).astype(int), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sanity checks for Labels\n",
    "def check_labels(index):\n",
    "    for column in label_df:\n",
    "        print(column)\n",
    "        print(label_df[[column]].iloc[index][0])\n",
    "        print(f\"Length: {len(label_df[[column]].iloc[index][0])}\")\n",
    "        print()\n",
    "    \n",
    "# check_labels(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sanity checks for sentences\n",
    "def check_sentence(index):\n",
    "    for column in input_df:\n",
    "        print(column)\n",
    "        print(input_df[[column]].iloc[index][0])\n",
    "        print(f\"Length: {len(input_df[[column]].iloc[index][0])}\")\n",
    "\n",
    "        print()\n",
    "    \n",
    "# check_sentence(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To prepare a single sentence\n",
    "index = 0\n",
    "\n",
    "#Convert lists torch tensors\n",
    "data_tensor = torch.tensor(input_df[['Input']].iloc[index][0]).unsqueeze(0)\n",
    "mask_tensor = torch.tensor(input_df[['Attention_Mask']].iloc[index][0]).unsqueeze(0)\n",
    "\n",
    "#This doesn't need to be a tensor since the labels will be used with the linear classifier outside ALBERT\n",
    "tags_output = label_df[['Output_Class']].iloc[index][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To prepare all sentences\n",
    "\n",
    "# Convert the list of lists into tensors\n",
    "data_matrix = torch.tensor([x[0] for x in input_df[['Input']].values])\n",
    "mask_matrix = torch.tensor([x[0] for x in input_df[['Attention_Mask']].values])\n",
    "\n",
    "#This doesn't need to be a tensor since the labels will be used with the linear classifier outside ALBERT\n",
    "tags_matrix = np.array([x[0] for x in label_df[['Output_Class']].values])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model\n",
    "from transformers import AutoModel\n",
    "model = AutoModel.from_pretrained('KB/bert-base-swedish-cased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function takes a single input sentence and mask and generates an embedding for all the tokens in the sentence, using the cpu\n",
    "\"\"\"\n",
    "def get_embeddings_with_cpu(data_tensor, mask_tensor):\n",
    "    embeddings = model.forward(input_ids=data_tensor,\n",
    "        attention_mask=mask_tensor,\n",
    "        head_mask=None)\n",
    "    print(embeddings[0].shape)\n",
    "    \n",
    "    return embeddings[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: GeForce GTX 1050 Ti\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1001/1001 [02:40<00:00,  6.23it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 1001 into shape (20000,50,newaxis)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-432178c7cbd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0membedding_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_embeddings_with_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-432178c7cbd3>\u001b[0m in \u001b[0;36mget_embeddings_with_gpu\u001b[0;34m(data_matrix, mask_matrix, batch_size)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# Merge the batches we've generated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0membedding_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_holder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Final embedding generated with shape {embedding_matrix.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 1001 into shape (20000,50,newaxis)"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Getting the embeddings for all the data with a cpu is possible.\n",
    "But it takes a lot of time.\n",
    "Using a GPU is faster.\n",
    "But you have to be careful so you dont go beyond the memory your card can handle.\n",
    "50 input sentences takes 2803MB on my computer.\n",
    "So, we split the input into batches and regenerate it into a complete embedding matrix afterwards\n",
    "\n",
    "        # The embedding matrix is a three-dimensional tensor corresponding to Sentence, Words, Embeddings\n",
    "        # embeddings[5][4][:] is thus the embedding of the fourth word in the fifth sentence\n",
    "\n",
    "\"\"\"\n",
    "def get_embeddings_with_gpu(data_matrix, mask_matrix, batch_size):\n",
    "    # Load the device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "    # Set the model to use the device\n",
    "    model.cuda()\n",
    "    \n",
    "    num_items = data_matrix.shape[0]\n",
    "    num_loops = int(num_items/batch_size) + 1\n",
    "    \n",
    "    start = 0\n",
    "    end = batch_size\n",
    "    \n",
    "    data_holder = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    #while end <= num_items:\n",
    "    for i in trange(num_loops):\n",
    "        #print(f\"Working on items {start} to {end} out of {num_items}\")\n",
    "        \n",
    "        # Split the data into batches\n",
    "        data_batch = data_matrix[start:end]\n",
    "        mask_batch = mask_matrix[start:end]\n",
    "        \n",
    "        # Move the data onto the GPU\n",
    "        data_batch = data_batch.to(device)\n",
    "        mask_batch = mask_batch.to(device)\n",
    "        \n",
    "        # Generate the embeddings for this batch\n",
    "        batch_embedding = model.forward(input_ids=data_batch,\n",
    "            attention_mask=mask_batch,\n",
    "            head_mask=None)[0]\n",
    "        #print(f\"Embedding generated with shape {batch_embedding.shape}\")\n",
    "    \n",
    "        # Make it an ordinary np array instead of a torch\n",
    "        batch_embedding = np.array(batch_embedding.tolist())\n",
    "        data_holder.append(batch_embedding)\n",
    "        \n",
    "        #Move to next batch\n",
    "        start += batch_size\n",
    "        end += batch_size\n",
    "    \n",
    "    \n",
    "    # Merge the batches we've generated\n",
    "    embedding_matrix = np.array(data_holder).reshape(num_items, length, -1)\n",
    "    print(f\"Final embedding generated with shape {embedding_matrix.shape}\")\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "embedding_matrix = get_embeddings_with_gpu(data_matrix, mask_matrix, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(embedding_matrix).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Format the labels\n",
    "tags_matrix = np.array([x[0] for x in label_df[['Output_Class']].values])\n",
    "print(tags_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the classifier on the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "embeddings = embedding_matrix.reshape(10000, -1)\n",
    "entities = tags_matrix.reshape(10000,)\n",
    "\n",
    "print(embeddings.shape)\n",
    "print(entities.shape)\n",
    "X = embeddings\n",
    "y = entities\n",
    "\n",
    "clf = LogisticRegression(random_state=0).fit(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".thesis",
   "language": "python",
   "name": ".thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
