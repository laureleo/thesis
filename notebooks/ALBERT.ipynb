{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data\n",
    "df = pd.read_pickle(\"../data/sentence_labels\")\n",
    "\n",
    "#Work on a subset for testing purposes\n",
    "input_df = df[['Sentence']].head(200).copy()\n",
    "label_df = df[['Labels']].head(200).copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting the input and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Format the input\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from transformers import AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prepares the sentences and labels for use with BERT-type models.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def format_data(input_df, label_df):\n",
    "    #Load swedish tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"KB/bert-base-swedish-cased-ner\")\n",
    "    \n",
    "    # Use WORDPIECE tokenization\n",
    "    #input_df['Tokenized'] = input_df[['Sentence']].apply(lambda x: tokenizer.tokenize(x[0]), axis=1)\n",
    "    # Ensure no named entities have been split apart\n",
    "\n",
    "    # use WORD tokenization\n",
    "    input_df['Tokenized'] = input_df[['Sentence']].apply(lambda x: x[0], axis=1)\n",
    "    label_df['Tokenized'] = label_df[['Labels']].apply(lambda x: x[0], axis=1)\n",
    "\n",
    "    # Replace words with integers, add the special [CLS] and [SEP] tokens\n",
    "    input_df['Integerized'] = input_df[['Tokenized']].apply(lambda x: tokenizer.encode(x[0], add_special_tokens=True), axis=1)\n",
    "    label_df['Integerized'] = label_df[['Tokenized']].apply(lambda x: tokenizer.encode(x[0], add_special_tokens=True), axis=1)\n",
    "\n",
    "    # Pad and truncate all sentences so they are the same length\n",
    "    length = 50\n",
    "    input_df['Input'] = input_df[['Integerized']].apply(lambda x: pad_sequences(x, maxlen=length, dtype=\"long\", truncating=\"post\", padding=\"post\")[0], axis=1)\n",
    "    label_df['Output_Class'] = label_df[['Integerized']].apply(lambda x: pad_sequences(x, maxlen=length, dtype=\"long\", truncating=\"post\", padding=\"post\")[0], axis=1)\n",
    "\n",
    "    # Create attention mask. Attention is 0 for padding, else 1\n",
    "    input_df['Attention_Mask'] = input_df[['Input']].apply(lambda x: (x[0] != 0).astype(int), axis=1)\n",
    "    \n",
    "    return input_df, label_df\n",
    "\n",
    "input_df, label_df = format_data(input_df, label_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sanity checks for Labels\n",
    "def check_labels(index):\n",
    "    for column in label_df:\n",
    "        print(column)\n",
    "        print(label_df[[column]].iloc[index][0])\n",
    "        print(f\"Length: {len(label_df[[column]].iloc[index][0])}\")\n",
    "        print()\n",
    "    \n",
    "# check_labels(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sanity checks for sentences\n",
    "def check_sentence(index):\n",
    "    for column in input_df:\n",
    "        print(column)\n",
    "        print(input_df[[column]].iloc[index][0])\n",
    "        print(f\"Length: {len(input_df[[column]].iloc[index][0])}\")\n",
    "\n",
    "        print()\n",
    "    \n",
    "# check_sentence(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To prepare a single sentence\n",
    "index = 0\n",
    "\n",
    "#Convert lists torch tensors\n",
    "data_tensor = torch.tensor(input_df[['Input']].iloc[index][0]).unsqueeze(0)\n",
    "mask_tensor = torch.tensor(input_df[['Attention_Mask']].iloc[index][0]).unsqueeze(0)\n",
    "\n",
    "#The tags don't need to be a tensor since the labels will be used with the linear classifier outside ALBERT\n",
    "tags_output = label_df[['Output_Class']].iloc[index][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To prepare all sentences\n",
    "\n",
    "# Convert the list of lists into tensors\n",
    "data_matrix = torch.tensor([x[0] for x in input_df[['Input']].values])\n",
    "mask_matrix = torch.tensor([x[0] for x in input_df[['Attention_Mask']].values])\n",
    "\n",
    "#This doesn't need to be a tensor since the labels will be used with the linear classifier outside ALBERT\n",
    "tags_matrix = np.array([x[0] for x in label_df[['Output_Class']].values])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model\n",
    "from transformers import AutoModel\n",
    "model = AutoModel.from_pretrained('KB/bert-base-swedish-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function takes a single input sentence and mask and generates an embedding for all the tokens in the sentence, using the cpu\n",
    "\"\"\"\n",
    "def get_embeddings_with_cpu(data_tensor, mask_tensor):\n",
    "    embeddings = model.forward(input_ids=data_tensor,\n",
    "        attention_mask=mask_tensor,\n",
    "        head_mask=None)\n",
    "    print(embeddings[0].shape)\n",
    "    \n",
    "    return embeddings[0]\n",
    "#embedding_matrix = get_embeddings_with_cpu(data_matrix[:10], mask_matrix[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For dealing with large amounts of data, a GPU is much faster\n",
    "\n",
    "The resulting embedding matrix is a three-dimensional tensor corresponding to [Sentence][Words][Embeddings]\n",
    "embeddings[5][4][:] is thus the embedding of the fourth word in the fifth sentence\n",
    "\"\"\"\n",
    "\n",
    "def get_embeddings_with_gpu(data_matrix, mask_matrix):\n",
    "    # Load the device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #print(f\"Using device: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "    # Set the model to use the device\n",
    "    model.cuda()\n",
    "\n",
    "    # Move the data onto the GPU\n",
    "    data_matrix = data_matrix.to(device)\n",
    "    mask_matrix = mask_matrix.to(device)\n",
    "\n",
    "    # Generate embeddings\n",
    "    matrix_embedding = model.forward(input_ids=data_matrix,\n",
    "        attention_mask=mask_matrix,\n",
    "        head_mask=None)[0]\n",
    "    #print(f\"Embedding generated with shape {batch_embedding.shape}\")\n",
    "\n",
    "    # Make it an ordinary np array instead of a torch\n",
    "    matrix_embedding = np.array(matrix_embedding.tolist())\n",
    "\n",
    "    return matrix_embedding\n",
    "\n",
    "#embedding_matrix = get_embeddings_with_gpu(data_matrix[:10], mask_matrix[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Most people won't be able to load all the data onto the GPU at once however, so it's better to do it in batches.\n",
    "(50 input sentences take 2803MB on my computer, for example).\n",
    "\n",
    "This method batchifies and stitches together the batches \n",
    "\"\"\"\n",
    "def get_embeddings_with_gpu_batch(data_matrix, mask_matrix, batch_size):\n",
    "    num_items = data_matrix.shape[0]\n",
    "    num_loops = int(np.ceil(num_items/batch_size))\n",
    "    \n",
    "    start = 0\n",
    "    end = batch_size\n",
    "    data_holder = []\n",
    "    \n",
    "    for i in trange(num_loops):        \n",
    "        # Split the data into batches\n",
    "        data_batch = data_matrix[start:end]\n",
    "        mask_batch = mask_matrix[start:end]\n",
    "        \n",
    "        #Get the embedding for the batch\n",
    "        batch_embedding = get_embeddings_with_gpu(data_batch, mask_batch)\n",
    "\n",
    "        data_holder.append(batch_embedding)\n",
    "        \n",
    "        #Move to next batch\n",
    "        start += batch_size\n",
    "        end += batch_size\n",
    "    \n",
    "    # Merge the batches we've generated\n",
    "    embedding_matrix = np.vstack(data_holder)\n",
    "\n",
    "    print(f\"Final embedding generated with shape {embedding_matrix.shape}\")\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "embedding_matrix = get_embeddings_with_gpu_batch(data_matrix, mask_matrix, 50)\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Right now our data is grouped by sentence. Since we'll do classification on a token-level however,\n",
    "# we join the sentences to get a list of tokens and their embeddings\n",
    "input_data = embedding_matrix.reshape(embedding_matrix.shape[0]*embedding_matrix.shape[1],-1)\n",
    "\n",
    "input_data.shape\n",
    "\n",
    "# Also, for convenience we create a token-embedding mapping\n",
    "cols = [\"Token\", \"Embedding\"]\n",
    "token_embedding_df = pd.DataFrame(columns=cols)\n",
    "token_embedding_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#token_embedding_df['Token'] = input_df[['Sentence']].apply(lambda x: x, axis=0)\n",
    "\n",
    "#token_embedding_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And of course, we do the same for the labels\n",
    "\n",
    "tags_matrix = np.array([x[0] for x in label_df[['Output_Class']].values])\n",
    "print(tags_matrix.shape)\n",
    "\n",
    "#Since we are dealing with token-level classification, break, unravel the sentence grouping we have\n",
    "token_labels = tags_matrix.reshape(-1,1)\n",
    "token_labels = pd.DataFrame(token_labels)\n",
    "\n",
    "#Since we are dealing with a multiclass problem, switch to one-hot encoding\n",
    "one_hot_token_labels = pd.get_dummies(token_labels[0])\n",
    "\n",
    "one_hot_token_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some last checks on the data\n",
    "num_dimensions = input_data.shape[1]\n",
    "num_classes = one_hot_token_labels.shape[1]\n",
    "print(num_dimensions,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For conveniece, let's save the generated embeddings so we can skip the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 0.8\n",
    "\n",
    "split = int(np.ceil(0.8*input_data.shape[0]))\n",
    "\n",
    "train_x = input_data[:split]\n",
    "train_y = one_hot_token_labels[:split]\n",
    "\n",
    "test_x = input_data[split:]\n",
    "test_y = one_hot_token_labels[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the classifier on the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple baseline model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        Dense(8, input_dim=num_dimensions, activation='relu'))\n",
    "    model.add(\n",
    "        Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = baseline_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model, setting aside 20% data for validation\n",
    "history = model.fit(\n",
    "    train_x,\n",
    "    train_y,\n",
    "    epochs=20,\n",
    "    batch_size=10,\n",
    "    validation_split=0.2)\n",
    "\n",
    "print('\\nhistory dict:', history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "training_df = pd.DataFrame()\n",
    "training_df['Accuracy'] = history.history['accuracy']\n",
    "training_df[\"Validation Accuracy\"] = history.history['val_accuracy']\n",
    "training_df['Loss'] = history.history['loss']\n",
    "training_df[\"Val Loss\"] = history.history['val_loss']\n",
    "\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "HISTORY_NAME = \"history_\" + str(date.today()) + \"_epochs_\" +str(training_df.shape[0])\n",
    "HISTORY_NAME\n",
    "\n",
    "# Save the history object for posterity\n",
    "training_df.to_pickle('../data/' + HISTORY_NAME)\n",
    "\n",
    "sns.set()\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "sns.lineplot(data = training_df, ax = ax)\n",
    "plt.ylabel('Metric')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "#estimator = KerasClassifier(build_fn=baseline_model, epochs=20, batch_size=5, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Use 10 fold cross-validation to reduce risk of bias\n",
    "#kfold = KFold(n_splits=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#results = cross_val_score(estimator, train_x, train_y, cv=kfold, verbose=1)\n",
    "#print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('\\n# Evaluate on test data')\n",
    "#results = model.evaluate(test_x, test_y)\n",
    "#print('test loss, test acc:', results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print('\\n# Generate predictions for 3 samples')\n",
    "predictions = model.predict(test_x[:3])\n",
    "print('predictions shape:', predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This method takes a sentence and does NER on it\n",
    "\"\"\"\n",
    "def NER(sentence):\n",
    "    #Sentence\n",
    "    s = sentence\n",
    "    print(f\"Sentence: {s}\")\n",
    "    \n",
    "    #Tokens\n",
    "    t = tokenizer.encode(s, add_special_tokens=True)\n",
    "    print(f\"Integer representation: {t}\")\n",
    "    \n",
    "    #Padded\n",
    "    p = pad_sequences([t], maxlen=length, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "    \n",
    "    #Mask\n",
    "    m = (p[0] != 0).astype(int)\n",
    "    \n",
    "    #Tensors\n",
    "    tp = torch.tensor(p)\n",
    "    tm = torch.tensor(m)\n",
    "    \n",
    "    print(f\"Embedding: {tm}\")\n",
    "    \n",
    "    #Encoding\n",
    "    e = get_embeddings_with_cpu(tp, tm)\n",
    "    print(f\"Embedding: {tp}\")\n",
    "    \n",
    "s = \"Hej, Nynäshamn är min stad\"\n",
    "NER(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".thesis",
   "language": "python",
   "name": ".thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
