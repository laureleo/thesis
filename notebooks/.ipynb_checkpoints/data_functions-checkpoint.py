
import pandas as pd
import numpy as np
from keras.preprocessing.sequence import pad_sequences
from transformers import AutoTokenizer
import torch

"""
Prepares the sentences and labels for use with BERT-type models.

input_df, label_df = format_data_for_BERT(input_df, label_df)
"""
def format_data_for_BERT(input_df, label_df):
    #Load swedish tokenizer
    tokenizer = AutoTokenizer.from_pretrained("KB/bert-base-swedish-cased-ner")
    
    # Use WORDPIECE tokenization
    #input_df['Tokenized'] = input_df[['Sentence']].apply(lambda x: tokenizer.tokenize(x[0]), axis=1)
    # Ensure no named entities have been split apart

    # use WORD tokenization
    input_df['Tokenized'] = input_df[['Sentence']].apply(lambda x: x[0], axis=1)
    label_df['Tokenized'] = label_df[['Labels']].apply(lambda x: x[0], axis=1)

    # Replace words with integers, add the special [CLS] and [SEP] tokens
    input_df['Integerized'] = input_df[['Tokenized']].apply(lambda x: tokenizer.encode(x[0], add_special_tokens=True), axis=1)
    label_df['Integerized'] = label_df[['Tokenized']].apply(lambda x: tokenizer.encode(x[0], add_special_tokens=True), axis=1)

    # Pad and truncate all sentences so they are the same length
    length = 50
    input_df['Input'] = input_df[['Integerized']].apply(lambda x: pad_sequences(x, maxlen=length, dtype="long", truncating="post", padding="post")[0], axis=1)
    label_df['Output_Class'] = label_df[['Integerized']].apply(lambda x: pad_sequences(x, maxlen=length, dtype="long", truncating="post", padding="post")[0], axis=1)

    # Create attention mask. Attention is 0 for padding, else 1
    input_df['Attention_Mask'] = input_df[['Input']].apply(lambda x: (x[0] != 0).astype(int), axis=1)
    
    #Convert input data to tensors
    data_matrix = torch.tensor([x[0] for x in input_df[['Input']].values])
    mask_matrix = torch.tensor([x[0] for x in input_df[['Attention_Mask']].values])

    #Create the output_clas matrix. Doesn't need to be a tensor, since it won't go into BERT
    label_matrix = np.array([x[0] for x in label_df[['Output_Class']].values])
    
    return data_matrix, mask_matrix, label_matrix

"""
The embeddings generated by the BERT model is grouped by sentence.
Since we will classify tokens, not sentences, 
input_data, output_data = format_data_for_NER(embedding_matrix, label_df)
"""
def format_data_for_NER(embedding_matrix, label_df):
    #Reshape the embeddings so it's a list of tokens and their embeddings
    input_data = embedding_matrix.reshape(embedding_matrix.shape[0]*embedding_matrix.shape[1],-1)
    
    #Since we are dealing with token-level classification, break, unravel the sentence grouping we have
    token_labels = tags_matrix.reshape(-1,1)
    token_labels = pd.DataFrame(token_labels)

    #Since we are dealing with a multiclass problem, switch to one-hot encoding
    one_hot_token_labels = pd.get_dummies(token_labels[0])

    return input_data, one_hot_token_labels

