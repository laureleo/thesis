{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Following the guide for BERT as practice\n",
    "https://www.depends-on-the-definition.com/named-entity-recognition-with-bert/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Token_ID</th>\n",
       "      <th>Entities</th>\n",
       "      <th>Entity_ID</th>\n",
       "      <th>Attention_Mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I sin första reaktion på Sovjetledarens varnin...</td>\n",
       "      <td>[135, 243, 578, 10540, 68, 3380, 7245, 49796, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, LOC, O, PRS, O, O, O,...</td>\n",
       "      <td>[11, 11, 11, 11, 11, 11, 11, 11, 16, 11, 14, 1...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  \\\n",
       "0  I sin första reaktion på Sovjetledarens varnin...   \n",
       "\n",
       "                                            Token_ID  \\\n",
       "0  [135, 243, 578, 10540, 68, 3380, 7245, 49796, ...   \n",
       "\n",
       "                                            Entities  \\\n",
       "0  [O, O, O, O, O, O, O, O, LOC, O, PRS, O, O, O,...   \n",
       "\n",
       "                                           Entity_ID  \\\n",
       "0  [11, 11, 11, 11, 11, 11, 11, 11, 16, 11, 14, 1...   \n",
       "\n",
       "                                      Attention_Mask  \n",
       "0  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(\"../data/suc3_formatted\")\n",
    "df.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into training and validation sets.\n",
    "\n",
    "#Albert takes both the sentence tokens (as ids), the labels (as ids) and attention masks as input, so we split those as well.\n",
    "r=random_state = 2018\n",
    "t=test_size = 0.1\n",
    "\n",
    "tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(df['Token_ID'].to_list(), df['Entity_ID'].to_list(), random_state=r, test_size=t)\n",
    "tr_masks, val_masks, _, _ = train_test_split(df['Attention_Mask'].to_list(), df['Token_ID'].to_list(),random_state=r, test_size=t)\n",
    "y = tr_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#Convert to tensors\n",
    "\n",
    "tr_inputs = torch.tensor(tr_inputs)\n",
    "val_inputs = torch.tensor(val_inputs)\n",
    "tr_tags = torch.tensor(tr_tags)\n",
    "val_tags = torch.tensor(val_tags)\n",
    "tr_masks = torch.tensor(tr_masks)\n",
    "val_masks = torch.tensor(val_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert it all into a tensordataset \n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#Set batch size\n",
    "bs = 1\n",
    "\n",
    "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n",
    "\n",
    "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distilbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import modeling_distilbert as md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class = ppb.DistilBertModel\n",
    "tokenizer_class = ppb.DistilBertTokenizer\n",
    "pretrained_weights = 'distilbert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model/tokenizer\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and process all sentences together as a batch\n",
    "# That is, replace each unique word with a corresponding id to get a list of lists\n",
    "tokenized = batch_1[0].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pad to ensure the same length\n",
    "\n",
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "\n",
    "np.array(padded).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that input is all the same, tell the model to ignore the padding when it sees it.\n",
    "#This is attention\n",
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to tensors\n",
    "input_ids = torch.tensor(padded)  \n",
    "attention_mask = torch.tensor(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The results of the processing ends up in last_hidden_states.\n",
    "#TODO what does torch.no_grad() do?\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's have a look at the final output\n",
    "\n",
    "\"\"\"\n",
    "We get a 2000 x 59 x 768 tensor\n",
    "\n",
    "The first dimension is the sentence\n",
    "The second dimension is the word\n",
    "The third dimension is the hidden state\n",
    "\n",
    "last_hidden_states[0][:,0,:]\n",
    "All rows (sentences), The first word(the prepended CLS token), all hidden states)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "last_hidden_states[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert classifies sentences by generating a [CLS] (classification) token and prepending it to the output sentences.\n",
    "# This token is an embedding for the entire sentence\n",
    "features = last_hidden_states[0][:,0,:].numpy()\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So we have an embedding for a sentence.\n",
    "#And we have a classification for each sentence.\n",
    "#Then we're back to familiar territory\n",
    "labels = batch_1[1]\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's just use basic logistic regression\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf.score(test_features, test_labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".thesis",
   "language": "python",
   "name": ".thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
