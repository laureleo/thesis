{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A separate notebook for dealing with all data\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_file():\n",
    "    data_type = np.uint8\n",
    "    g = np.memmap('../data/ONE_HOT_LABELS.dat', dtype=data_type, shape=(74165, 100, 33))\n",
    "    \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed_file():\n",
    "    data_type = np.float64\n",
    "    f = np.memmap('../data/MMAP_MATRIX.dat', dtype=data_type, shape=(74165, 100, 768))\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, Bidirectional, TimeDistributed, Dropout, Dense\n",
    "from keras.models import Model, Input\n",
    "def bilstm_model():\n",
    "    \"\"\"\n",
    "    The model selection was very basic. Testing was performed on the same 5000 sentences with number of units = 100, 200, 300, 400\n",
    "    Each subsequent increase in units massivly increased complexity for minor gains in performance.\n",
    "    10% of the data was used for validation\n",
    "    \n",
    "    500\n",
    "    loss: 0.0266 - accuracy: 0.9924 - val_loss: 0.0321 - val_accuracy: 0.9909\n",
    "    \n",
    "    400\n",
    "    loss: 0.0279 - accuracy: 0.9921 - val_loss: 0.0326 - val_accuracy: 0.9906\n",
    "    \n",
    "    300\n",
    "    loss: 0.0296 - accuracy: 0.9918 - val_loss: 0.0333 - val_accuracy: 0.9905\n",
    "    \n",
    "    200\n",
    "    loss: 0.0328 - accuracy: 0.9911 - val_loss: 0.0349 - val_accuracy: 0.9904\n",
    "    \n",
    "    100\n",
    "    loss: 0.0403 - accuracy: 0.9897 - val_loss: 0.0394 - val_accuracy: 0.9899\n",
    "    \n",
    "    For all choices of unit, validation accuracy peaked around 4-7 epochs\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    #Define the input shape. Each datapoint is a sentence consisting of seqlength words, each word 784dims\n",
    "    input = Input(shape=(train_x.shape[1], train_x.shape[2]))\n",
    "    # Pass it through a bidirectional lstm\n",
    "    model = Bidirectional(LSTM(units=200, return_sequences=True, recurrent_dropout=0.1))(input)\n",
    "\n",
    "    # add a timedistributed layer\n",
    "    out = TimeDistributed(Dense(train_y.shape[2], activation=\"softmax\"))(model)  # softmax output layer\n",
    "    \n",
    "    model = Model(input, out)\n",
    "    \n",
    "    # Compile it\n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    print(model.summary())\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, batch_size=32, shuffle=False):\n",
    "        'Initialization'\n",
    "        self.batch_size = batch_size\n",
    "        self.list_IDs = list_IDs\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Denotes the number of batches per epoch\n",
    "        A common way to decide this is simply total amount of samples over batch size\n",
    "        the batch size you can handle is, of course, determined by your computer, so set that accordingly\n",
    "        \"\"\"\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        This function is responsible for grabbing the indices to load for each batch.\n",
    "        And then calling the data generator for those ids\n",
    "        \"\"\"\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"\n",
    "        If desired, shuffles the indices after each epoch\n",
    "        \"\"\"\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        \"\"\"\n",
    "        This function brings data from disk into RAM.\n",
    "        It is called once per batch, and thus brings in one batch of data.\n",
    "        \"\"\"\n",
    "        X = get_embed_file()[list_IDs_temp,:,:]\n",
    "        y = get_label_file()[list_IDs_temp,:,:]\n",
    "        \n",
    "        # We take the data at the specified indices and retrieve it\n",
    "        return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(classifier_function, nr_folds, indices):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        Classifier_function; a function that generates a classifier with the desired attributes. Will be used to reset the classifier between folds\n",
    "        nr_folds, how many folds to split into\n",
    "        indices, a list of the indices we will pass to our data generators\n",
    "        \n",
    "    Output:\n",
    "        For each fold, three saved files.\n",
    "        One containing the model\n",
    "        One containing the model weights\n",
    "        One containing the history dataframe\n",
    "    \"\"\"\n",
    "    fold_size = int(np.floor(len(indices)/nr_folds))\n",
    "    \n",
    "    for i in range(nr_folds):\n",
    "        print(f\"Working on fold {i + 1} of {nr_folds}\")\n",
    "        \n",
    "        #Select the indices to validate on\n",
    "        validation_indices = indices[i * fold_size : (i+1) * fold_size]\n",
    "        \n",
    "        #Select all indices except those we validate on for training\n",
    "        train_indices =  [x for x in indices if x not in validation_indices]\n",
    "        \n",
    "        print(f\"Training on {len(train_indices)}, validating on {len(validation_indices)}\")\n",
    "        \n",
    "        print(\"Resetting model...\")\n",
    "        classifier = classifier_function()\n",
    "\n",
    "        print(\"Created datagenerators\")\n",
    "        dataGen = DataGenerator(train_indices)\n",
    "        valiGen = DataGenerator(validation_indices)\n",
    "        \n",
    "        print(\"Commencing training\")\n",
    "        history = classifier.fit_generator(generator=dataGen,\n",
    "                                           validation_data=valiGen,\n",
    "                                           epochs = 10)\n",
    "        \n",
    "        print(\"TRAINING COMPLETE\")\n",
    "        \n",
    "        #Pickle the training data in case it will be neeeded\n",
    "        training_df = pd.DataFrame()\n",
    "        for metric in history.history.keys():\n",
    "            training_df[metric] = history.history[metric]\n",
    "            \n",
    "        training_df.to_pickle(f'../data/fold_{i}_history')\n",
    "        print(f\"SAVING HISTORY '../data/fold_{i}_history'\")\n",
    "        \n",
    "        # Serialize the model in case we'll need it\n",
    "        model_json = classifier.to_json()\n",
    "        with open(f\"../data/fold_{i}_model.json\", \"w+\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "        print(f\"SAVING MODEL ../data/fold_{i}_model.json\")\n",
    "\n",
    "            \n",
    "        # Serialize weights to HDF5, becuse we WILL need them\n",
    "        classifier.save_weights(f'../data/fold_{i}_weights.h5')\n",
    "\n",
    "        print(f\"SAVING MODEL WEIGHTS AS ../data/fold_{i}_weights.h5\")\n",
    "    \n",
    "    print(\"PROCESS COMPLETED\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUBSET TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the memory mapped arrays\n",
    "embedding_pointer = get_embed_file()\n",
    "label_pointer = get_label_file()\n",
    "\n",
    "#Select a subset\n",
    "train_x = embedding_pointer[20000:21000]\n",
    "train_y = label_pointer[20000:21000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 100, 768)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 100, 400)          1550400   \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 100, 33)           13233     \n",
      "=================================================================\n",
      "Total params: 1,563,633\n",
      "Trainable params: 1,563,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1800 samples, validate on 200 samples\n",
      "Epoch 1/10\n",
      "1800/1800 [==============================] - 9s 5ms/step - loss: 0.5296 - accuracy: 0.8776 - val_loss: 0.2036 - val_accuracy: 0.9466\n",
      "Epoch 2/10\n",
      "1800/1800 [==============================] - 9s 5ms/step - loss: 0.1367 - accuracy: 0.9689 - val_loss: 0.1172 - val_accuracy: 0.9718\n",
      "Epoch 3/10\n",
      "1800/1800 [==============================] - 9s 5ms/step - loss: 0.0897 - accuracy: 0.9792 - val_loss: 0.0862 - val_accuracy: 0.9797\n",
      "Epoch 4/10\n",
      "1800/1800 [==============================] - 10s 5ms/step - loss: 0.0670 - accuracy: 0.9838 - val_loss: 0.0682 - val_accuracy: 0.9837\n",
      "Epoch 5/10\n",
      "1800/1800 [==============================] - 11s 6ms/step - loss: 0.0538 - accuracy: 0.9865 - val_loss: 0.0586 - val_accuracy: 0.9848\n",
      "Epoch 6/10\n",
      "1800/1800 [==============================] - 9s 5ms/step - loss: 0.0455 - accuracy: 0.9881 - val_loss: 0.0523 - val_accuracy: 0.9866\n",
      "Epoch 7/10\n",
      "1800/1800 [==============================] - 9s 5ms/step - loss: 0.0396 - accuracy: 0.9895 - val_loss: 0.0479 - val_accuracy: 0.9870\n",
      "Epoch 8/10\n",
      "1800/1800 [==============================] - 9s 5ms/step - loss: 0.0351 - accuracy: 0.9905 - val_loss: 0.0450 - val_accuracy: 0.9874\n",
      "Epoch 9/10\n",
      "1800/1800 [==============================] - 9s 5ms/step - loss: 0.0311 - accuracy: 0.9914 - val_loss: 0.0427 - val_accuracy: 0.9883\n",
      "Epoch 10/10\n",
      "1800/1800 [==============================] - 11s 6ms/step - loss: 0.0281 - accuracy: 0.9919 - val_loss: 0.0415 - val_accuracy: 0.9883\n",
      "\n",
      "history dict: dict_keys(['val_loss', 'val_accuracy', 'loss', 'accuracy'])\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "classifier = bilstm_model()\n",
    "\n",
    "# Train, setting some data aside for evaluation\n",
    "history = classifier.fit(\n",
    "        train_x,\n",
    "        train_y,\n",
    "        epochs=10,\n",
    "        batch_size=50,\n",
    "        validation_split=0.1)\n",
    "print('\\nhistory dict:', history.history.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CROSSVAL TESTING\n",
    "\n",
    "The test indices will be the last 10 % and we won't touch those at all for the moment.\n",
    "\n",
    "We will make use of 5-fold cross validation to reduce bias in our results. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_indices = [i for i in range(get_embed_file().shape[0])]\n",
    "\n",
    "#Test consist of the last 10 %\n",
    "TEST = sentence_indices[66749:]\n",
    "CROSSVAL = sentence_indices[:66749]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on fold 1 of 5\n",
      "Training on 53400, validating on 13349\n",
      "Resetting model...\n",
      "Model: \"model_27\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_27 (InputLayer)        (None, 100, 768)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_27 (Bidirectio (None, 100, 400)          1550400   \n",
      "_________________________________________________________________\n",
      "time_distributed_27 (TimeDis (None, 100, 33)           13233     \n",
      "=================================================================\n",
      "Total params: 1,563,633\n",
      "Trainable params: 1,563,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Created datagenerators\n",
      "Commencing training\n",
      "Epoch 1/10\n",
      "1668/1668 [==============================] - 227s 136ms/step - loss: 0.0492 - accuracy: 0.9871 - val_loss: 0.0261 - val_accuracy: 0.9920\n",
      "Epoch 2/10\n",
      "1668/1668 [==============================] - 240s 144ms/step - loss: 0.0247 - accuracy: 0.9926 - val_loss: 0.0218 - val_accuracy: 0.9927\n",
      "Epoch 3/10\n",
      "1668/1668 [==============================] - 262s 157ms/step - loss: 0.0209 - accuracy: 0.9935 - val_loss: 0.0184 - val_accuracy: 0.9931\n",
      "Epoch 4/10\n",
      "1668/1668 [==============================] - 238s 143ms/step - loss: 0.0178 - accuracy: 0.9944 - val_loss: 0.0185 - val_accuracy: 0.9933\n",
      "Epoch 5/10\n",
      "1668/1668 [==============================] - 231s 138ms/step - loss: 0.0147 - accuracy: 0.9953 - val_loss: 0.0221 - val_accuracy: 0.9934\n",
      "Epoch 6/10\n",
      "1668/1668 [==============================] - 226s 136ms/step - loss: 0.0117 - accuracy: 0.9963 - val_loss: 0.0254 - val_accuracy: 0.9936\n",
      "Epoch 7/10\n",
      "1668/1668 [==============================] - 225s 135ms/step - loss: 0.0087 - accuracy: 0.9972 - val_loss: 0.0220 - val_accuracy: 0.9935\n",
      "Epoch 8/10\n",
      "1668/1668 [==============================] - 222s 133ms/step - loss: 0.0061 - accuracy: 0.9981 - val_loss: 0.0288 - val_accuracy: 0.9936\n",
      "Epoch 9/10\n",
      "1668/1668 [==============================] - 223s 134ms/step - loss: 0.0042 - accuracy: 0.9987 - val_loss: 0.0233 - val_accuracy: 0.9934\n",
      "Epoch 10/10\n",
      "1668/1668 [==============================] - 222s 133ms/step - loss: 0.0030 - accuracy: 0.9990 - val_loss: 0.0254 - val_accuracy: 0.9934\n",
      "TRAINING COMPLETE\n",
      "SAVING HISTORY '../data/fold_0_history'\n",
      "SAVING MODEL ../data/fold_0_model.json\n",
      "SAVING MODEL WEIGHTS AS ../data/fold_0_weights.h5\n",
      "Working on fold 2 of 5\n",
      "Training on 53400, validating on 13349\n",
      "Resetting model...\n",
      "Model: \"model_28\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_28 (InputLayer)        (None, 100, 768)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_28 (Bidirectio (None, 100, 400)          1550400   \n",
      "_________________________________________________________________\n",
      "time_distributed_28 (TimeDis (None, 100, 33)           13233     \n",
      "=================================================================\n",
      "Total params: 1,563,633\n",
      "Trainable params: 1,563,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Created datagenerators\n",
      "Commencing training\n",
      "Epoch 1/10\n",
      "1668/1668 [==============================] - 222s 133ms/step - loss: 0.0467 - accuracy: 0.9875 - val_loss: 0.0229 - val_accuracy: 0.9921\n",
      "Epoch 2/10\n",
      "1668/1668 [==============================] - 223s 134ms/step - loss: 0.0243 - accuracy: 0.9927 - val_loss: 0.0190 - val_accuracy: 0.9927\n",
      "Epoch 3/10\n",
      "1668/1668 [==============================] - 223s 134ms/step - loss: 0.0211 - accuracy: 0.9936 - val_loss: 0.0184 - val_accuracy: 0.9931\n",
      "Epoch 4/10\n",
      "1668/1668 [==============================] - 223s 134ms/step - loss: 0.0178 - accuracy: 0.9945 - val_loss: 0.0179 - val_accuracy: 0.9934\n",
      "Epoch 5/10\n",
      "1668/1668 [==============================] - 224s 134ms/step - loss: 0.0146 - accuracy: 0.9954 - val_loss: 0.0184 - val_accuracy: 0.9935\n",
      "Epoch 6/10\n",
      "1668/1668 [==============================] - 225s 135ms/step - loss: 0.0113 - accuracy: 0.9964 - val_loss: 0.0172 - val_accuracy: 0.9934\n",
      "Epoch 7/10\n",
      "1668/1668 [==============================] - 226s 135ms/step - loss: 0.0082 - accuracy: 0.9973 - val_loss: 0.0185 - val_accuracy: 0.9935\n",
      "Epoch 8/10\n",
      "1668/1668 [==============================] - 227s 136ms/step - loss: 0.0056 - accuracy: 0.9982 - val_loss: 0.0188 - val_accuracy: 0.9934\n",
      "Epoch 9/10\n",
      "1668/1668 [==============================] - 224s 134ms/step - loss: 0.0038 - accuracy: 0.9988 - val_loss: 0.0202 - val_accuracy: 0.9933\n",
      "Epoch 10/10\n",
      "1668/1668 [==============================] - 223s 134ms/step - loss: 0.0027 - accuracy: 0.9992 - val_loss: 0.0205 - val_accuracy: 0.9936\n",
      "TRAINING COMPLETE\n",
      "SAVING HISTORY '../data/fold_1_history'\n",
      "SAVING MODEL ../data/fold_1_model.json\n",
      "SAVING MODEL WEIGHTS AS ../data/fold_1_weights.h5\n",
      "Working on fold 3 of 5\n",
      "Training on 53400, validating on 13349\n",
      "Resetting model...\n",
      "Model: \"model_29\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_29 (InputLayer)        (None, 100, 768)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_29 (Bidirectio (None, 100, 400)          1550400   \n",
      "_________________________________________________________________\n",
      "time_distributed_29 (TimeDis (None, 100, 33)           13233     \n",
      "=================================================================\n",
      "Total params: 1,563,633\n",
      "Trainable params: 1,563,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Created datagenerators\n",
      "Commencing training\n",
      "Epoch 1/10\n",
      "1668/1668 [==============================] - 221s 132ms/step - loss: 0.0477 - accuracy: 0.9874 - val_loss: 0.0262 - val_accuracy: 0.9923\n",
      "Epoch 2/10\n",
      "1668/1668 [==============================] - 222s 133ms/step - loss: 0.0247 - accuracy: 0.9926 - val_loss: 0.0240 - val_accuracy: 0.9931\n",
      "Epoch 3/10\n",
      "1668/1668 [==============================] - 222s 133ms/step - loss: 0.0212 - accuracy: 0.9935 - val_loss: 0.0236 - val_accuracy: 0.9935\n",
      "Epoch 4/10\n",
      "1668/1668 [==============================] - 224s 134ms/step - loss: 0.0180 - accuracy: 0.9944 - val_loss: 0.0242 - val_accuracy: 0.9934\n",
      "Epoch 5/10\n",
      "1668/1668 [==============================] - 223s 134ms/step - loss: 0.0150 - accuracy: 0.9953 - val_loss: 0.0247 - val_accuracy: 0.9937\n",
      "Epoch 6/10\n",
      "1668/1668 [==============================] - 224s 134ms/step - loss: 0.0119 - accuracy: 0.9962 - val_loss: 0.0267 - val_accuracy: 0.9936\n",
      "Epoch 7/10\n",
      "1668/1668 [==============================] - 223s 134ms/step - loss: 0.0087 - accuracy: 0.9972 - val_loss: 0.0316 - val_accuracy: 0.9935\n",
      "Epoch 8/10\n",
      "1668/1668 [==============================] - 223s 134ms/step - loss: 0.0062 - accuracy: 0.9980 - val_loss: 0.0317 - val_accuracy: 0.9935\n",
      "Epoch 9/10\n",
      "1668/1668 [==============================] - 223s 134ms/step - loss: 0.0043 - accuracy: 0.9986 - val_loss: 0.0395 - val_accuracy: 0.9931\n",
      "Epoch 10/10\n",
      "1668/1668 [==============================] - 223s 134ms/step - loss: 0.0033 - accuracy: 0.9990 - val_loss: 0.0394 - val_accuracy: 0.9938\n",
      "TRAINING COMPLETE\n",
      "SAVING HISTORY '../data/fold_2_history'\n",
      "SAVING MODEL ../data/fold_2_model.json\n",
      "SAVING MODEL WEIGHTS AS ../data/fold_2_weights.h5\n",
      "Working on fold 4 of 5\n",
      "Training on 53400, validating on 13349\n",
      "Resetting model...\n",
      "Model: \"model_30\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_30 (InputLayer)        (None, 100, 768)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_30 (Bidirectio (None, 100, 400)          1550400   \n",
      "_________________________________________________________________\n",
      "time_distributed_30 (TimeDis (None, 100, 33)           13233     \n",
      "=================================================================\n",
      "Total params: 1,563,633\n",
      "Trainable params: 1,563,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Created datagenerators\n",
      "Commencing training\n",
      "Epoch 1/10\n",
      "1668/1668 [==============================] - 224s 134ms/step - loss: 0.0461 - accuracy: 0.9877 - val_loss: 0.0168 - val_accuracy: 0.9921\n",
      "Epoch 2/10\n",
      "1668/1668 [==============================] - 224s 134ms/step - loss: 0.0244 - accuracy: 0.9927 - val_loss: 0.0133 - val_accuracy: 0.9929\n",
      "Epoch 3/10\n",
      "1668/1668 [==============================] - 224s 134ms/step - loss: 0.0207 - accuracy: 0.9937 - val_loss: 0.0131 - val_accuracy: 0.9933\n",
      "Epoch 4/10\n",
      "1668/1668 [==============================] - 226s 136ms/step - loss: 0.0178 - accuracy: 0.9945 - val_loss: 0.0131 - val_accuracy: 0.9935\n",
      "Epoch 5/10\n",
      "1668/1668 [==============================] - 225s 135ms/step - loss: 0.0143 - accuracy: 0.9955 - val_loss: 0.0129 - val_accuracy: 0.9936\n",
      "Epoch 6/10\n",
      "1668/1668 [==============================] - 224s 134ms/step - loss: 0.0114 - accuracy: 0.9964 - val_loss: 0.0157 - val_accuracy: 0.9937\n",
      "Epoch 7/10\n",
      "1668/1668 [==============================] - 226s 136ms/step - loss: 0.0083 - accuracy: 0.9973 - val_loss: 0.0138 - val_accuracy: 0.9937\n",
      "Epoch 8/10\n",
      "1668/1668 [==============================] - 225s 135ms/step - loss: 0.0057 - accuracy: 0.9982 - val_loss: 0.0169 - val_accuracy: 0.9935\n",
      "Epoch 9/10\n",
      "1668/1668 [==============================] - 225s 135ms/step - loss: 0.0039 - accuracy: 0.9987 - val_loss: 0.0142 - val_accuracy: 0.9937\n",
      "Epoch 10/10\n",
      "1668/1668 [==============================] - 225s 135ms/step - loss: 0.0028 - accuracy: 0.9992 - val_loss: 0.0162 - val_accuracy: 0.9935\n",
      "TRAINING COMPLETE\n",
      "SAVING HISTORY '../data/fold_3_history'\n",
      "SAVING MODEL ../data/fold_3_model.json\n",
      "SAVING MODEL WEIGHTS AS ../data/fold_3_weights.h5\n",
      "Working on fold 5 of 5\n",
      "Training on 53400, validating on 13349\n",
      "Resetting model...\n",
      "Model: \"model_31\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_31 (InputLayer)        (None, 100, 768)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_31 (Bidirectio (None, 100, 400)          1550400   \n",
      "_________________________________________________________________\n",
      "time_distributed_31 (TimeDis (None, 100, 33)           13233     \n",
      "=================================================================\n",
      "Total params: 1,563,633\n",
      "Trainable params: 1,563,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Created datagenerators\n",
      "Commencing training\n",
      "Epoch 1/10\n",
      "1668/1668 [==============================] - 227s 136ms/step - loss: 0.0473 - accuracy: 0.9874 - val_loss: 0.0197 - val_accuracy: 0.9921\n",
      "Epoch 2/10\n",
      "1668/1668 [==============================] - 227s 136ms/step - loss: 0.0243 - accuracy: 0.9927 - val_loss: 0.0195 - val_accuracy: 0.9926\n",
      "Epoch 3/10\n",
      "1668/1668 [==============================] - 226s 135ms/step - loss: 0.0208 - accuracy: 0.9937 - val_loss: 0.0174 - val_accuracy: 0.9931\n",
      "Epoch 4/10\n",
      "1668/1668 [==============================] - 227s 136ms/step - loss: 0.0175 - accuracy: 0.9945 - val_loss: 0.0185 - val_accuracy: 0.9934\n",
      "Epoch 5/10\n",
      "1668/1668 [==============================] - 225s 135ms/step - loss: 0.0144 - accuracy: 0.9955 - val_loss: 0.0194 - val_accuracy: 0.9935\n",
      "Epoch 6/10\n",
      "1668/1668 [==============================] - 227s 136ms/step - loss: 0.0111 - accuracy: 0.9964 - val_loss: 0.0199 - val_accuracy: 0.9936\n",
      "Epoch 7/10\n",
      "1668/1668 [==============================] - 226s 135ms/step - loss: 0.0080 - accuracy: 0.9974 - val_loss: 0.0181 - val_accuracy: 0.9936\n",
      "Epoch 8/10\n",
      "1668/1668 [==============================] - 226s 135ms/step - loss: 0.0056 - accuracy: 0.9982 - val_loss: 0.0228 - val_accuracy: 0.9937\n",
      "Epoch 9/10\n",
      "1668/1668 [==============================] - 228s 137ms/step - loss: 0.0038 - accuracy: 0.9988 - val_loss: 0.0210 - val_accuracy: 0.9937\n",
      "Epoch 10/10\n",
      "1668/1668 [==============================] - 228s 137ms/step - loss: 0.0028 - accuracy: 0.9992 - val_loss: 0.0258 - val_accuracy: 0.9935\n",
      "TRAINING COMPLETE\n",
      "SAVING HISTORY '../data/fold_4_history'\n",
      "SAVING MODEL ../data/fold_4_model.json\n",
      "SAVING MODEL WEIGHTS AS ../data/fold_4_weights.h5\n",
      "PROCESS COMPLETED\n",
      "Time needed for CV on entire dataset = 3.1499764224555755 hours\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "cross_validate(bilstm_model, 5, CROSSVAL)\n",
    "end = time.time()\n",
    "print(f\"Time needed for CV on entire dataset = {(end-start)/3600} hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vTime needed for CV on entire dataset = 3.1499764224555755 hours\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD STORED INFORMATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history\n",
    "history_df = pd.read_pickle('../data/fold_0_history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_26\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_26 (InputLayer)        (None, 100, 768)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_26 (Bidirectio (None, 100, 400)          1550400   \n",
      "_________________________________________________________________\n",
      "time_distributed_26 (TimeDis (None, 100, 33)           13233     \n",
      "=================================================================\n",
      "Total params: 1,563,633\n",
      "Trainable params: 1,563,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# Model and its weights\n",
    "classifier = bilstm_model()\n",
    "\n",
    "# load weights into new model\n",
    "classifier.load_weights(\"../data/fold_0_weights.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluate on test data\n",
      "loss: 0.035807523876428604\n",
      "accuracy: 0.9876822829246521\n"
     ]
    }
   ],
   "source": [
    "\n",
    "evalGen = DataGenerator(TRAIN[:5000])\n",
    "end = time.time()\n",
    "print('\\nEvaluate on test data')\n",
    "results = classifier.evaluate_generator(evalGen)\n",
    "for i, metric in enumerate(classifier.metrics_names):\n",
    "    print(f\"{metric}: {results[i]}\")\n",
    "elapsed = start - end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.00024199485778808594\n"
     ]
    }
   ],
   "source": [
    "print(elapsed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".thesis",
   "language": "python",
   "name": ".thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
