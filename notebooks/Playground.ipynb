{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Following the guide for BERT as practice\n",
    "https://www.depends-on-the-definition.com/named-entity-recognition-with-bert/\n",
    "\n",
    "On using BERT as encoder\n",
    "https://towardsdatascience.com/word-embedding-using-bert-in-python-dd5a86c00342"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distilbert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control the model architecture configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertModel, DistilBertConfig\n",
    "\n",
    "# Initializing a DistilBERT configuration\n",
    "configuration = DistilBertConfig()\n",
    "\n",
    "# Initializing a model from the configuration\n",
    "model = DistilBertModel(configuration)\n",
    "\n",
    "# Accessing the model configuration\n",
    "configuration = model.config\n",
    "\n",
    "#You can control the model configuration by changing any parameter in the constructor\n",
    "\n",
    "#For example \n",
    "#configuration = DistilBertConfig(do_sample = True)\n",
    "#And then passing it to the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16f896341955423bbdb437b2f62c5e1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=895.0, style=ProgressStyle(description_â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading the model does not load any weights, only the configuration.\n",
    "model = DistilBertModel(configuration)\n",
    "\n",
    "# Use the from_pretrained() method to load weights, though naturally you have to use their configuration then\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1422, 1271, 1110, 7800, 102]\n",
      "[101, 1, 1, 1, 1, 102]\n",
      "[101, 152, 152, 152, 100, 102]\n",
      "tensor([[ 101, 1422, 1271, 1110, 7800,  102]])\n",
      "tensor([[101,   1,   1,   1,   1, 102]])\n",
      "tensor([[101, 152, 152, 152, 100, 102]])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')\n",
    "\n",
    "\n",
    "text = \"My name is Sophie\"\n",
    "mask = [1, 1, 1, 1]\n",
    "tags = [\"O\", \"O\", \"O\", \"PER\"]\n",
    "\n",
    "#Add special tokens to get cls and sep\n",
    "text_enc = tokenizer.encode(text, add_special_tokens=True)\n",
    "mask_enc = tokenizer.encode(mask, add_special_tokens=True)\n",
    "tags_enc = tokenizer.encode(tags, add_special_tokens=True)\n",
    "\n",
    "print(text_enc)\n",
    "print(mask_enc)\n",
    "print(tags_enc)\n",
    "\n",
    "text_enc_tensor = torch.tensor(text_enc).unsqueeze(0)\n",
    "mask_enc_tensor = torch.tensor(mask_enc).unsqueeze(0)\n",
    "tags_enc_tensor = torch.tensor(tags_enc).unsqueeze(0)\n",
    "\n",
    "print(text_enc_tensor)\n",
    "print(mask_enc_tensor)\n",
    "print(tags_enc_tensor)\n",
    "\n",
    "#Generate embeddings with BERT\n",
    "#Train token classification on 90 % of the embeddings, test on rest\n",
    "\n",
    "\n",
    "#input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
    "#outputs = model(input_ids)\n",
    "\n",
    "#last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.forward(input_ids=text_enc_tensor,\n",
    "    attention_mask=mask_enc_tensor,\n",
    "    head_mask=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4358,  0.0456,  0.0396,  ..., -0.1634,  0.2456, -0.0158],\n",
       "         [-0.1548,  0.0235,  0.6477,  ..., -0.1363, -0.0646,  0.1977],\n",
       "         [ 0.2925,  0.1372,  0.2723,  ...,  0.4394,  0.0764,  0.1033],\n",
       "         [ 0.1624,  0.0689,  0.3299,  ...,  0.2121,  0.1571,  0.1517],\n",
       "         [ 0.0784,  0.0850, -0.1499,  ...,  0.0408,  0.2438,  0.0382],\n",
       "         [ 1.0227, -0.0188,  0.1744,  ..., -0.0983,  0.9721, -0.0347]]],\n",
       "       grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Passing the sentence through the model, we get a \n",
    "embeddings[0].shape\n",
    "\n",
    "# There are 6 encodings, one for each token (our sentence + special cls and sep tokens)\n",
    "# Each one is represented in 768 embedding space\n",
    "\n",
    "embeddings[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On how BERT works\n",
    "https://towardsdatascience.com/bert-to-the-rescue-17671379687f\n",
    "https://yashuseth.blog/2019/06/12/bert-explained-faqs-understand-bert-working/\n",
    "Bert is, in the end, an encoder. If a rather sophisticated one.\n",
    "\n",
    "The INPUT to bert is a sequence of WORDPIECE pokens and two special tokens [CLS] and [SEP].\n",
    "\n",
    "[SEP] is\n",
    "[CLS] is\n",
    "WORDPIECES are\n",
    "\n",
    "An EXAMPLE INPUT is\n",
    "[[CLS], My, name, is, Sophie, [SEP], Yours, [?]]\n",
    "\n",
    "Passing it through BERT, we receive a rich encoding for each word\n",
    "[[CLS], E1, E2, E3, E4, [SEP], E5, E6]\n",
    "All we have to do then is to classify the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.forward(input_ids=text_enc_tensor,\n",
    "    attention_mask=mask_enc_tensor,\n",
    "    head_mask=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".thesis",
   "language": "python",
   "name": ".thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
